{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ex1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoavdana/ex1_DL/blob/main/ex1_copy2232.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3RC-HpHQ-Hs",
        "outputId": "5adc9f81-5135-4eeb-d97c-25a6d9b42b9a"
      },
      "source": [
        "!git clone https://github.com/yoavdana/ex1_DL.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ex1_DL'...\n",
            "remote: Enumerating objects: 109, done.\u001b[K\n",
            "remote: Counting objects: 100% (109/109), done.\u001b[K\n",
            "remote: Compressing objects: 100% (93/93), done.\u001b[K\n",
            "remote: Total 109 (delta 38), reused 48 (delta 9), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (109/109), 59.26 MiB | 30.10 MiB/s, done.\n",
            "Resolving deltas: 100% (38/38), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kew0gkAsRDWn"
      },
      "source": [
        "import torch \n",
        "import numpy as np\n",
        "import scipy.stats as si\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import f1_score\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "from time import time\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm import tqdm\n",
        "import pandas as pd"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knIQ-N2mRdQH"
      },
      "source": [
        "# Loading Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_PDI34NMRjw"
      },
      "source": [
        "Data Proccesing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66jPLZBeMMTK"
      },
      "source": [
        "SEQ_LENGTH = 20\n",
        "BATCH_SIZE = 64\n",
        "MAPPING = {'A': 0, 'R': 1, 'N': 2, 'D': 3, 'C': 4, 'E': 5, 'Q': 6, 'G': 7, 'H': 8, 'I': 9, 'L': 10, 'K': 11,\n",
        "              'M': 12, 'F': 13, 'P': 14, 'S': 15, 'T': 16, 'W': 17, 'Y': 18, 'V': 19}\n",
        "\n",
        "\n",
        "def data_to_input(sequence, pos_or_neg):\n",
        "    map=np.zeros((9, 20))\n",
        "    for i, seq in enumerate(sequence):\n",
        "        map[i,MAPPING[seq]]+=1\n",
        "    map = map.flatten()\n",
        "    if pos_or_neg==None:\n",
        "        return map\n",
        "    return np.concatenate([map, np.array([pos_or_neg])])\n",
        "\n",
        "def read_data(filename, pos_or_neg):\n",
        "    file = open(filename, 'r')\n",
        "    lines=file.readlines()\n",
        "    data = np.zeros((len(lines), 181))\n",
        "    if pos_or_neg == None:\n",
        "        data = np.zeros((len(lines), 180))\n",
        "    for i, line in enumerate(lines):\n",
        "        input = data_to_input(line.replace('\\n', ''), pos_or_neg)\n",
        "        data[i] = input\n",
        "    return data\n",
        "\n",
        "# def bootstrap(DATA,size,NUMBER_OF_BATCHS):\n",
        "\n",
        "#     new_DATA=np.zeros((size,181))\n",
        "#     N=DATA.shape[0]\n",
        "#     batch_size=N//NUMBER_OF_BATCHS\n",
        "#     for i in range(NUMBER_OF_BATCHS):\n",
        "\n",
        "#         random = np.random.randint(batch_size*i,batch_size*(i+1), size=size//NUMBER_OF_BATCHS)\n",
        "#         new_DATA[((size//NUMBER_OF_BATCHS)*i):(size//NUMBER_OF_BATCHS)*(i+1), :] = DATA[random, :]\n",
        "#     return new_DATA\n",
        "\n",
        "\n",
        "def bootstrap(data,size,NUMBER_OF_BATCHS):\n",
        "\n",
        "    N=data.shape[0]\n",
        "    batch_size=N//NUMBER_OF_BATCHS\n",
        "    for i in range(NUMBER_OF_BATCHS):\n",
        "        random = np.random.randint(batch_size*i,batch_size*(i+1), size=size//NUMBER_OF_BATCHS)\n",
        "        data =np.vstack([DATA,DATA[random, :]])\n",
        "    return data\n",
        "\n",
        "\n",
        "def data_pre_pros(filename_pos,filename_neg):\n",
        "\n",
        "    neg_data=read_data(filename_neg, 0)\n",
        "    pos_data=read_data(filename_pos, 1)\n",
        "    \n",
        "    neg_data_train = neg_data[:int(len(neg_data)*0.9)]\n",
        "    neg_data_test = neg_data[int(len(neg_data)*0.9):]\n",
        "    pos_data_train = pos_data[:int(len(pos_data)*0.9)]\n",
        "    pos_data_test = pos_data[int(len(pos_data)*0.9):]\n",
        "    #pos_data_train = bootstrap(pos_data_train, int(BOOTSTRAP_SIZE*0.9), int(NUMBER_OF_BATCHS*0.9))\n",
        "    #pos_data_test = bootstrap(pos_data_test, int(BOOTSTRAP_SIZE*0.1), int(NUMBER_OF_BATCHS*0.1))\n",
        "\n",
        "    final_data_train = np.concatenate([neg_data_train, pos_data_train])\n",
        "    final_data_test = np.concatenate([neg_data_test, pos_data_test])\n",
        "    return final_data_train, final_data_test\n",
        "\n",
        "\n",
        "def shuffle_data(data_Xy):\n",
        "    np.random.shuffle(data_Xy)\n",
        "    return data_Xy[:,:180],data_Xy[:,-1]\n",
        "\n",
        "\n",
        "def spike_seq(filename):\n",
        "    with open(filename) as f:\n",
        "        lines = f.readlines()[0]\n",
        "        print(lines)\n",
        "        predeict=list()\n",
        "\n",
        "        if len(lines) == 9:\n",
        "            map = np.zeros((9, 20))\n",
        "            for i, seq in enumerate(lines):\n",
        "                map[i, MAPPING[seq]] += 1\n",
        "            map = map.flatten()\n",
        "            predeict.append(map)\n",
        "        else:\n",
        "            for i in range(len(lines)-9):\n",
        "                map = np.zeros((9, 20))\n",
        "                for i, seq in enumerate(lines[i:i+9]):\n",
        "                    map[i, MAPPING[seq]] += 1\n",
        "                map = map.flatten()\n",
        "                predeict.append(map)\n",
        "        \n",
        "        return np.array(predeict)\n"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYNbU_SaRgzZ"
      },
      "source": [
        "filename_pos='/content/ex1_DL/resorces/pos_A0201.txt'\n",
        "filename_neg='/content/ex1_DL/resorces/neg_A0201.txt'\n",
        "\n",
        "train_set, test_set=data_pre_pros(filename_pos,filename_neg)\n",
        "train_x, train_y = train_set[:,:180], train_set[:,-1]\n",
        "test_x, test_y = test_set[:,:180], test_set[:,-1]\n",
        "\n",
        "train_target = torch.from_numpy(train_y.astype(np.int64))\n",
        "train = torch.from_numpy(train_x.astype(np.float32)) \n",
        "train_tensor = torch.utils.data.TensorDataset(train, train_target) \n",
        "train_dataloader = torch.utils.data.DataLoader(dataset = train_tensor, batch_size = BATCH_SIZE, shuffle = True)\n",
        "\n",
        "test_target = torch.from_numpy(test_y.astype(np.int64))\n",
        "test = torch.from_numpy(test_x.astype(np.float32)) \n",
        "test_tensor = torch.utils.data.TensorDataset(test, test_target) \n",
        "test_dataloader = torch.utils.data.DataLoader(dataset = test_tensor, batch_size = BATCH_SIZE, shuffle = True)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkOKJ3pkTRut"
      },
      "source": [
        "# Evaluation functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08W0jQWQTnvo"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "# def calculate_accuracy(y_pred, y):\n",
        "#     top_pred = y_pred.argmax(1, keepdim = True)\n",
        "#     correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
        "#     acc = correct.float() / y.shape[0]\n",
        "#     return acc\n",
        "\n",
        "\n",
        "# def calculate_accuracy(y_pred, y):\n",
        "#     top_pred = y_pred.argmax(1, keepdim = True)\n",
        "#     f1 = f1_score(y_pred,y)\n",
        "#     return f1\n",
        "    \n",
        "\n",
        "def calculate_accuracy(y_true, y_pred, is_training=False):\n",
        "    if y_pred.ndim == 2:\n",
        "        y_pred = y_pred.argmax(dim=1)\n",
        "       \n",
        "   \n",
        "    tp = (y_true * y_pred).sum().to(torch.float32)\n",
        "    tn = ((1 - y_true) * (1 - y_pred)).sum().to(torch.float32)\n",
        "    fp = ((1 - y_true) * y_pred).sum().to(torch.float32)\n",
        "    fn = (y_true * (1 - y_pred)).sum().to(torch.float32)\n",
        "   \n",
        "    epsilon = 1e-7\n",
        "   \n",
        "    precision = tp / (tp + fp + epsilon)\n",
        "    recall = tp / (tp + fn + epsilon)\n",
        "    accuracy = (tp + tn) / (tp + tn + fp + fn + epsilon)\n",
        "   \n",
        "    f1 = 2* (precision*recall) / (precision + recall + epsilon)\n",
        "    f1.requires_grad = is_training\n",
        "    return accuracy"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhkcZrZYU9jQ"
      },
      "source": [
        "## Network architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zU3l7iJlVG6k"
      },
      "source": [
        "# Network parameters\n",
        "INPUT=180\n",
        "INPUT_1=25\n",
        "INPUT_2=15\n",
        "INPUT_3=8\n",
        "INPUT_4=4\n",
        "INPUT_5=16\n",
        "INPUT_6=8\n",
        "INPUT_7=4\n",
        "OUTPUT=2\n",
        "P_DROPOUT=0.1\n",
        "P_DROPOUT_2=0.15\n",
        "\n",
        "LEARNNING_RATE=0.00008\n",
        "\n",
        "BOOTSTRAP_SIZE=15000\n",
        "NUMBER_OF_BATCHS=150\n",
        "\n",
        "EPOCHS = 50"
      ],
      "execution_count": 455,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9TzVlOwVcaI"
      },
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Network, self).__init__()\n",
        "\n",
        "        self.input_fc = nn.Linear(INPUT, INPUT_1)\n",
        "        \n",
        "        self.hidden_1_fc = nn.Linear(INPUT_1, INPUT_2)\n",
        "        \n",
        "        self.hidden_2_fc = nn.Linear(INPUT_2, INPUT_3)\n",
        "\n",
        "        self.hidden_3_fc = nn.Linear(INPUT_3, INPUT_4)\n",
        "\n",
        "        self.output_fc = nn.Linear(INPUT_4, OUTPUT)\n",
        "\n",
        "        self.dropout = nn.Dropout(P_DROPOUT)\n",
        "\n",
        "        \n",
        "        self.batch_norm_1 = nn.BatchNorm1d(INPUT_1)\n",
        "        self.batch_norm_2 = nn.BatchNorm1d(INPUT_2)\n",
        "        self.batch_norm_3 = nn.BatchNorm1d(INPUT_3)\n",
        "        self.batch_norm_4 = nn.BatchNorm1d(INPUT_4)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "   \n",
        "\n",
        "        h_1 = F.relu(self.batch_norm_1(self.input_fc(x)))\n",
        "\n",
        "        h_1=self.dropout(h_1)\n",
        "\n",
        "        h_2 = F.relu(self.batch_norm_2(self.hidden_1_fc(h_1)))\n",
        "\n",
        "        # h_2=self.dropout(h_2)\n",
        "\n",
        "        h_3 = F.relu(self.batch_norm_3(self.hidden_2_fc(h_2)))\n",
        "\n",
        "        #h_3=self.dropout2(h_3)\n",
        "\n",
        "        h_4 = F.relu(self.batch_norm_4(self.hidden_3_fc(h_3)))\n",
        "\n",
        "        y_pred = self.output_fc(h_4)\n",
        "\n",
        "        return y_pred, h_4\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Network().to(device)\n",
        "optimizer=optim.Adam(model.parameters(), lr=LEARNNING_RATE)\n",
        "weights = [train_set.shape[0]/(2*2.4*np.sum(train_set[:, -1])), 2.6*np.sum(train_set[:, -1])/(2*(train_set.shape[0]-np.sum(train_set[:, -1])))] #as class distribution\n",
        "weights.reverse()\n",
        "class_weights = torch.FloatTensor(weights).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)"
      ],
      "execution_count": 456,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mb8kMOWPiG_"
      },
      "source": [
        "# Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uFUj0MVPgoO",
        "outputId": "fc93fc54-99ef-4986-ddca-b67d928b26a4"
      },
      "source": [
        "start_time=time()\n",
        "\n",
        "for epoch in range(20):\n",
        "    running_loss=0.0\n",
        "    pbar=tqdm(iterable=train_dataloader)\n",
        "    for i,batch in enumerate(pbar):\n",
        "      batch=[item.to(device) for  item in batch]\n",
        "      sequences, labels=batch\n",
        "      # zero the parameter gradients\n",
        "      optimizer.zero_grad()\n",
        "      # forward \n",
        "      outputs, _ = model(sequences)\n",
        "      #calculate the loss between the target and the actuals\n",
        "      loss= criterion(input=outputs, target=labels)\n",
        "      #Gradient calculation uisng backward pass\n",
        "      loss.backward()\n",
        "      # update the weights\n",
        "      optimizer.step()\n",
        "      running_loss+=loss.item()\n",
        "      pbar.set_postfix(loss=running_loss/(i+1))\n",
        "    pbar.close()\n",
        "    print('epoch %d -Loss %.3f' % (epoch +1,running_loss/(len(train_tensor)/BATCH_SIZE)))      \n",
        "    running_loss = 0.0\n",
        "\n",
        "print(\"Time for training using PyTorch %f\" %(time()-start_time))"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 387/387 [00:03<00:00, 122.14it/s, loss=0.289]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1 -Loss 0.289\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 387/387 [00:03<00:00, 121.26it/s, loss=0.131]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2 -Loss 0.131\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 387/387 [00:03<00:00, 122.79it/s, loss=0.12]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3 -Loss 0.120\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 387/387 [00:03<00:00, 121.10it/s, loss=0.116]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 4 -Loss 0.116\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 387/387 [00:03<00:00, 121.53it/s, loss=0.114]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 5 -Loss 0.114\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 387/387 [00:03<00:00, 122.54it/s, loss=0.116]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 6 -Loss 0.116\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 387/387 [00:03<00:00, 122.28it/s, loss=0.11]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 7 -Loss 0.110\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 387/387 [00:03<00:00, 120.90it/s, loss=0.114]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 8 -Loss 0.114\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 387/387 [00:03<00:00, 120.51it/s, loss=0.109]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 9 -Loss 0.109\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 387/387 [00:03<00:00, 118.58it/s, loss=0.109]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 10 -Loss 0.110\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 387/387 [00:03<00:00, 117.21it/s, loss=0.111]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 11 -Loss 0.111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 387/387 [00:03<00:00, 119.94it/s, loss=0.11]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 12 -Loss 0.110\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 387/387 [00:03<00:00, 120.39it/s, loss=0.107]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 13 -Loss 0.107\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 387/387 [00:03<00:00, 120.04it/s, loss=0.109]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 14 -Loss 0.109\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 387/387 [00:03<00:00, 120.90it/s, loss=0.108]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 15 -Loss 0.108\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 387/387 [00:03<00:00, 119.06it/s, loss=0.106]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 16 -Loss 0.106\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 387/387 [00:03<00:00, 118.97it/s, loss=0.106]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 17 -Loss 0.106\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 387/387 [00:03<00:00, 117.70it/s, loss=0.108]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 18 -Loss 0.108\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 387/387 [00:03<00:00, 118.98it/s, loss=0.105]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 19 -Loss 0.105\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 387/387 [00:03<00:00, 120.47it/s, loss=0.106]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 20 -Loss 0.106\n",
            "Time for training using PyTorch 64.577247\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34cbmGnjpMtR"
      },
      "source": [
        "def train_model(model, train_dataloader, optimizer, criterion, device):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.train()\n",
        "\n",
        "    for i,batch in enumerate(train_dataloader):\n",
        "        batch=[item.to(device) for item in batch]\n",
        "        sequences, labels=batch\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "      \n",
        "        # forward \n",
        "        outputs, _ = model(sequences)\n",
        "        #calculate the loss between the target and the actuals\n",
        "        loss= criterion(input=outputs, target=labels)\n",
        "        #Gradient calculation uisng backward pass\n",
        "        acc=calculate_accuracy(labels, outputs)\n",
        "        loss.backward()\n",
        "        # update the weights\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(train_dataloader), epoch_acc / len(train_dataloader)\n",
        "\n",
        "\n",
        "def evaluate(model, test_dataloader, criterion, device):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.eval()\n",
        "\n",
        "    for i,batch in enumerate(test_dataloader):\n",
        "        batch=[item.to(device) for item in batch]\n",
        "        sequences, labels=batch\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "      \n",
        "        # forward \n",
        "        outputs, _ = model(sequences)\n",
        "        #calculate the loss between the target and the actuals\n",
        "        loss= criterion(input=outputs, target=labels)\n",
        "        #Gradient calculation uisng backward pass\n",
        "        acc=calculate_accuracy(labels, outputs)\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(test_dataloader), epoch_acc / len(test_dataloader)\n"
      ],
      "execution_count": 420,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "D7OQ_DBFrHs7",
        "outputId": "622e7b9f-8ca1-464d-d6cb-fdba63136057"
      },
      "source": [
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "print(f'weights: {weights}')\n",
        "train_per_ep=[]\n",
        "test_per_ep=[]\n",
        "test_acc = 0\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss, train_acc = train_model(model, train_dataloader, optimizer, criterion, device)\n",
        "    test_loss, test_acc = evaluate(model, test_dataloader,criterion, device)\n",
        "    \n",
        "    train_per_ep.append(train_loss)\n",
        "    test_per_ep.append(test_loss)\n",
        "    \n",
        "    print(f'Epoch: {epoch + 1:02}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')\n",
        "    print(f'Epoch: {epoch + 1:02}')\n",
        "    print(f'\\tTest Loss: {test_loss:.3f} | Test Acc: {test_acc * 100:.2f}%')\n",
        "\n",
        "train_per_ep=np.array(train_per_ep)\n",
        "test_per_ep=np.array(test_per_ep)\n",
        "epocs=np.arange(1,EPOCHS+1)\n",
        "print(np.mean(np.abs(np.array(train_per_ep) - np.array(test_per_ep))))\n",
        "plt.figure()\n",
        "plt.plot(epocs,train_per_ep)\n",
        "plt.plot(epocs,test_per_ep)\n",
        "plt.legend(['train loss','test loss'])\n",
        "plt.show()"
      ],
      "execution_count": 457,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 5,193 trainable parameters\n",
            "weights: [0.15871064331730333, 1.9147931376192246]\n",
            "Epoch: 01\n",
            "\tTrain Loss: 0.775 | Train Acc: 89.11%\n",
            "Epoch: 01\n",
            "\tTest Loss: 0.702 | Test Acc: 89.01%\n",
            "Epoch: 02\n",
            "\tTrain Loss: 0.694 | Train Acc: 88.72%\n",
            "Epoch: 02\n",
            "\tTest Loss: 0.670 | Test Acc: 88.18%\n",
            "Epoch: 03\n",
            "\tTrain Loss: 0.658 | Train Acc: 86.93%\n",
            "Epoch: 03\n",
            "\tTest Loss: 0.636 | Test Acc: 86.11%\n",
            "Epoch: 04\n",
            "\tTrain Loss: 0.618 | Train Acc: 82.14%\n",
            "Epoch: 04\n",
            "\tTest Loss: 0.579 | Test Acc: 80.29%\n",
            "Epoch: 05\n",
            "\tTrain Loss: 0.555 | Train Acc: 77.43%\n",
            "Epoch: 05\n",
            "\tTest Loss: 0.505 | Test Acc: 75.82%\n",
            "Epoch: 06\n",
            "\tTrain Loss: 0.485 | Train Acc: 76.61%\n",
            "Epoch: 06\n",
            "\tTest Loss: 0.446 | Test Acc: 78.73%\n",
            "Epoch: 07\n",
            "\tTrain Loss: 0.433 | Train Acc: 77.46%\n",
            "Epoch: 07\n",
            "\tTest Loss: 0.400 | Test Acc: 77.20%\n",
            "Epoch: 08\n",
            "\tTrain Loss: 0.395 | Train Acc: 78.51%\n",
            "Epoch: 08\n",
            "\tTest Loss: 0.361 | Test Acc: 81.38%\n",
            "Epoch: 09\n",
            "\tTrain Loss: 0.365 | Train Acc: 79.29%\n",
            "Epoch: 09\n",
            "\tTest Loss: 0.338 | Test Acc: 80.57%\n",
            "Epoch: 10\n",
            "\tTrain Loss: 0.338 | Train Acc: 80.69%\n",
            "Epoch: 10\n",
            "\tTest Loss: 0.315 | Test Acc: 81.17%\n",
            "Epoch: 11\n",
            "\tTrain Loss: 0.322 | Train Acc: 81.13%\n",
            "Epoch: 11\n",
            "\tTest Loss: 0.303 | Test Acc: 82.00%\n",
            "Epoch: 12\n",
            "\tTrain Loss: 0.306 | Train Acc: 81.99%\n",
            "Epoch: 12\n",
            "\tTest Loss: 0.289 | Test Acc: 81.75%\n",
            "Epoch: 13\n",
            "\tTrain Loss: 0.297 | Train Acc: 82.55%\n",
            "Epoch: 13\n",
            "\tTest Loss: 0.275 | Test Acc: 82.44%\n",
            "Epoch: 14\n",
            "\tTrain Loss: 0.287 | Train Acc: 82.73%\n",
            "Epoch: 14\n",
            "\tTest Loss: 0.267 | Test Acc: 84.07%\n",
            "Epoch: 15\n",
            "\tTrain Loss: 0.276 | Train Acc: 83.09%\n",
            "Epoch: 15\n",
            "\tTest Loss: 0.263 | Test Acc: 83.79%\n",
            "Epoch: 16\n",
            "\tTrain Loss: 0.269 | Train Acc: 83.54%\n",
            "Epoch: 16\n",
            "\tTest Loss: 0.253 | Test Acc: 82.10%\n",
            "Epoch: 17\n",
            "\tTrain Loss: 0.267 | Train Acc: 83.69%\n",
            "Epoch: 17\n",
            "\tTest Loss: 0.250 | Test Acc: 82.80%\n",
            "Epoch: 18\n",
            "\tTrain Loss: 0.262 | Train Acc: 84.01%\n",
            "Epoch: 18\n",
            "\tTest Loss: 0.244 | Test Acc: 84.58%\n",
            "Epoch: 19\n",
            "\tTrain Loss: 0.259 | Train Acc: 84.05%\n",
            "Epoch: 19\n",
            "\tTest Loss: 0.244 | Test Acc: 83.93%\n",
            "Epoch: 20\n",
            "\tTrain Loss: 0.254 | Train Acc: 84.42%\n",
            "Epoch: 20\n",
            "\tTest Loss: 0.243 | Test Acc: 84.26%\n",
            "Epoch: 21\n",
            "\tTrain Loss: 0.252 | Train Acc: 84.31%\n",
            "Epoch: 21\n",
            "\tTest Loss: 0.240 | Test Acc: 85.13%\n",
            "Epoch: 22\n",
            "\tTrain Loss: 0.242 | Train Acc: 84.89%\n",
            "Epoch: 22\n",
            "\tTest Loss: 0.240 | Test Acc: 85.60%\n",
            "Epoch: 23\n",
            "\tTrain Loss: 0.245 | Train Acc: 84.91%\n",
            "Epoch: 23\n",
            "\tTest Loss: 0.241 | Test Acc: 85.85%\n",
            "Epoch: 24\n",
            "\tTrain Loss: 0.242 | Train Acc: 84.72%\n",
            "Epoch: 24\n",
            "\tTest Loss: 0.242 | Test Acc: 84.22%\n",
            "Epoch: 25\n",
            "\tTrain Loss: 0.239 | Train Acc: 85.15%\n",
            "Epoch: 25\n",
            "\tTest Loss: 0.239 | Test Acc: 84.51%\n",
            "Epoch: 26\n",
            "\tTrain Loss: 0.235 | Train Acc: 85.22%\n",
            "Epoch: 26\n",
            "\tTest Loss: 0.235 | Test Acc: 84.48%\n",
            "Epoch: 27\n",
            "\tTrain Loss: 0.234 | Train Acc: 85.22%\n",
            "Epoch: 27\n",
            "\tTest Loss: 0.236 | Test Acc: 84.21%\n",
            "Epoch: 28\n",
            "\tTrain Loss: 0.238 | Train Acc: 85.39%\n",
            "Epoch: 28\n",
            "\tTest Loss: 0.233 | Test Acc: 85.16%\n",
            "Epoch: 29\n",
            "\tTrain Loss: 0.235 | Train Acc: 85.35%\n",
            "Epoch: 29\n",
            "\tTest Loss: 0.229 | Test Acc: 85.34%\n",
            "Epoch: 30\n",
            "\tTrain Loss: 0.225 | Train Acc: 85.88%\n",
            "Epoch: 30\n",
            "\tTest Loss: 0.239 | Test Acc: 86.72%\n",
            "Epoch: 31\n",
            "\tTrain Loss: 0.225 | Train Acc: 85.89%\n",
            "Epoch: 31\n",
            "\tTest Loss: 0.231 | Test Acc: 86.01%\n",
            "Epoch: 32\n",
            "\tTrain Loss: 0.228 | Train Acc: 85.99%\n",
            "Epoch: 32\n",
            "\tTest Loss: 0.240 | Test Acc: 86.62%\n",
            "Epoch: 33\n",
            "\tTrain Loss: 0.228 | Train Acc: 85.97%\n",
            "Epoch: 33\n",
            "\tTest Loss: 0.246 | Test Acc: 86.40%\n",
            "Epoch: 34\n",
            "\tTrain Loss: 0.222 | Train Acc: 86.16%\n",
            "Epoch: 34\n",
            "\tTest Loss: 0.243 | Test Acc: 86.98%\n",
            "Epoch: 35\n",
            "\tTrain Loss: 0.220 | Train Acc: 86.27%\n",
            "Epoch: 35\n",
            "\tTest Loss: 0.234 | Test Acc: 85.52%\n",
            "Epoch: 36\n",
            "\tTrain Loss: 0.218 | Train Acc: 86.59%\n",
            "Epoch: 36\n",
            "\tTest Loss: 0.245 | Test Acc: 87.48%\n",
            "Epoch: 37\n",
            "\tTrain Loss: 0.224 | Train Acc: 86.28%\n",
            "Epoch: 37\n",
            "\tTest Loss: 0.251 | Test Acc: 87.28%\n",
            "Epoch: 38\n",
            "\tTrain Loss: 0.218 | Train Acc: 86.59%\n",
            "Epoch: 38\n",
            "\tTest Loss: 0.254 | Test Acc: 87.16%\n",
            "Epoch: 39\n",
            "\tTrain Loss: 0.213 | Train Acc: 86.73%\n",
            "Epoch: 39\n",
            "\tTest Loss: 0.244 | Test Acc: 85.75%\n",
            "Epoch: 40\n",
            "\tTrain Loss: 0.215 | Train Acc: 86.91%\n",
            "Epoch: 40\n",
            "\tTest Loss: 0.252 | Test Acc: 87.17%\n",
            "Epoch: 41\n",
            "\tTrain Loss: 0.213 | Train Acc: 86.94%\n",
            "Epoch: 41\n",
            "\tTest Loss: 0.251 | Test Acc: 87.31%\n",
            "Epoch: 42\n",
            "\tTrain Loss: 0.209 | Train Acc: 87.00%\n",
            "Epoch: 42\n",
            "\tTest Loss: 0.232 | Test Acc: 85.46%\n",
            "Epoch: 43\n",
            "\tTrain Loss: 0.212 | Train Acc: 87.22%\n",
            "Epoch: 43\n",
            "\tTest Loss: 0.253 | Test Acc: 87.20%\n",
            "Epoch: 44\n",
            "\tTrain Loss: 0.212 | Train Acc: 87.01%\n",
            "Epoch: 44\n",
            "\tTest Loss: 0.241 | Test Acc: 85.49%\n",
            "Epoch: 45\n",
            "\tTrain Loss: 0.209 | Train Acc: 87.43%\n",
            "Epoch: 45\n",
            "\tTest Loss: 0.258 | Test Acc: 87.96%\n",
            "Epoch: 46\n",
            "\tTrain Loss: 0.204 | Train Acc: 87.62%\n",
            "Epoch: 46\n",
            "\tTest Loss: 0.249 | Test Acc: 86.26%\n",
            "Epoch: 47\n",
            "\tTrain Loss: 0.207 | Train Acc: 87.57%\n",
            "Epoch: 47\n",
            "\tTest Loss: 0.259 | Test Acc: 88.22%\n",
            "Epoch: 48\n",
            "\tTrain Loss: 0.208 | Train Acc: 87.46%\n",
            "Epoch: 48\n",
            "\tTest Loss: 0.242 | Test Acc: 87.08%\n",
            "Epoch: 49\n",
            "\tTrain Loss: 0.203 | Train Acc: 87.82%\n",
            "Epoch: 49\n",
            "\tTest Loss: 0.252 | Test Acc: 87.16%\n",
            "Epoch: 50\n",
            "\tTrain Loss: 0.204 | Train Acc: 87.82%\n",
            "Epoch: 50\n",
            "\tTest Loss: 0.269 | Test Acc: 88.51%\n",
            "0.02460590197538683\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lw7GZ0uXab_W",
        "outputId": "126b04cf-7adf-42f1-84f6-80e8a71ee1e3"
      },
      "source": [
        "seq=spike_seq('/content/ex1_DL/resorces/spike_acid.txt')\n",
        "print(len(seq))\n",
        "seq = torch.from_numpy(seq.astype('float32'))\n",
        "seq = seq.to(device)\n",
        "y_pred, _ = model(seq)\n",
        "z = y_pred\n",
        "my_softmax = nn.Softmax(dim=1)\n",
        "z = my_softmax(z)\n",
        "# print(len(torch.where(z[:,1] > 0.5)[0]) / len(z))\n",
        "# print(len(torch.where(z[:,0] > 0.5)[0]) / len(z))\n",
        "print(len(torch.where(z[:,1] > 0.96)[0]))\n",
        "\n"
      ],
      "execution_count": 430,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSSVLHSTQDLFLPFFSNVTWFHAIHVSGTNGTKRFDNPVLPFNDGVYFASTEKSNIIRGWIFGTTLDSKTQSLLIVNNATNVVIKVCEFQFCNDPFLGVYYHKNNKSWMESEFRVYSSANNCTFEYVSQPFLMDLEGKQGNFKNLREFVFKNIDGYFKIYSKHTPINLVRDLPQGFSALEPLVDLPIGINITRFQTLLALHRSYLTPGDSSSGWTAGAAAYYVGYLQPRTFLLKYNENGTITDAVDCALDPLSETKCTLKSFTVEKGIYQTSNFRVQPTESIVRFPNITNLCPFGEVFNATRFASVYAWNRKRISNCVADYSVLYNSASFSTFKCYGVSPTKLNDLCFTNVYADSFVIRGDEVRQIAPGQTGKIADYNYKLPDDFTGCVIAWNSNNLDSKVGGNYNYLYRLFRKSNLKPFERDISTEIYQAGSTPCNGVEGFNCYFPLQSYGFQPTNGVGYQPYRVVVLSFELLHAPATVCGPKKSTNLVKNKCVNFNFNGLTGTGVLTESNKKFLPFQQFGRDIADTTDAVRDPQTLEILDITPCSFGGVSVITPGTNTSNQVAVLYQDVNCTEVPVAIHADQLTPTWRVYSTGSNVFQTRAGCLIGAEHVNNSYECDIPIGAGICASYQTQTNSPRRARSVASQSIIAYTMSLGAENSVAYSNNSIAIPTNFTISVTTEILPVSMTKTSVDCTMYICGDSTECSNLLLQYGSFCTQLNRALTGIAVEQDKNTQEVFAQVKQIYKTPPIKDFGGFNFSQILPDPSKPSKRSFIEDLLFNKVTLADAGFIKQYGDCLGDIAARDLICAQKFNGLTVLPPLLTDEMIAQYTSALLAGTITSGWTFGAGAALQIPFAMQMAYRFNGIGVTQNVLYENQKLIANQFNSAIGKIQDSLSSTASALGKLQDVVNQNAQALNTLVKQLSSNFGAISSVLNDILSRLDKVEAEVQIDRLITGRLQSLQTYVTQQLIRAAEIRASANLAATKMSECVLGQSKRVDFCGKGYHLMSFPQSAPHGVVFLHVTYVPAQEKNFTTAPAICHDGKAHFPREGVFVSNGTHWFVTQRNFYEPQIITTDNTFVSGNCDVVIGIVNNTVYDPLQPELDSFKEELDKYFKNHTSPDVDLGDISGINASVVNIQKEIDRLNEVAKNLNESLIDLQELGKYEQYIKWPWYIWLGFIAGLIAIVMVTIMLCCMTSCCSCLKGCCSCGSCCKFDEDDSEPVLKGVKLHYT\n",
            "1264\n",
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bo3ITfw82-zz",
        "outputId": "9a0f32a4-2513-451d-d9b0-1d823207e76f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "seq=read_data('/content/ex1_DL/resorces/test_set.txt', None)\n",
        "print(len(seq))\n",
        "seq = torch.from_numpy(seq.astype('float32'))\n",
        "seq = seq.to(device)\n",
        "y_pred, _ = model(seq)\n",
        "z = y_pred\n",
        "my_softmax = nn.Softmax(dim=1)\n",
        "z = my_softmax(z)\n",
        "print(len(torch.where(z[:,1] > 0.5)[0]) / len(z))\n",
        "# print(len(torch.where(z[:,0] > 0.5)[0]) / len(z))\n"
      ],
      "execution_count": 432,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "213\n",
            "0.8544600938967136\n"
          ]
        }
      ]
    }
  ]
}