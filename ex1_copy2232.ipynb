{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ex1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoavdana/ex1_DL/blob/main/ex1_copy2232.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3RC-HpHQ-Hs",
        "outputId": "5adc9f81-5135-4eeb-d97c-25a6d9b42b9a"
      },
      "source": [
        "!git clone https://github.com/yoavdana/ex1_DL.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ex1_DL'...\n",
            "remote: Enumerating objects: 109, done.\u001b[K\n",
            "remote: Counting objects: 100% (109/109), done.\u001b[K\n",
            "remote: Compressing objects: 100% (93/93), done.\u001b[K\n",
            "remote: Total 109 (delta 38), reused 48 (delta 9), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (109/109), 59.26 MiB | 30.10 MiB/s, done.\n",
            "Resolving deltas: 100% (38/38), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kew0gkAsRDWn"
      },
      "source": [
        "import torch \n",
        "import numpy as np\n",
        "import scipy.stats as si\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import f1_score\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "from time import time\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm import tqdm\n",
        "import pandas as pd"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knIQ-N2mRdQH"
      },
      "source": [
        "# Loading Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_PDI34NMRjw"
      },
      "source": [
        "Data Proccesing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66jPLZBeMMTK"
      },
      "source": [
        "SEQ_LENGTH = 20\n",
        "BATCH_SIZE = 64\n",
        "MAPPING = {'A': 0, 'R': 1, 'N': 2, 'D': 3, 'C': 4, 'E': 5, 'Q': 6, 'G': 7, 'H': 8, 'I': 9, 'L': 10, 'K': 11,\n",
        "              'M': 12, 'F': 13, 'P': 14, 'S': 15, 'T': 16, 'W': 17, 'Y': 18, 'V': 19}\n",
        "\n",
        "\n",
        "def data_to_input(sequence, pos_or_neg):\n",
        "    map=np.zeros((9, 20))\n",
        "    for i, seq in enumerate(sequence):\n",
        "        map[i,MAPPING[seq]]+=1\n",
        "    map = map.flatten()\n",
        "    if pos_or_neg==None:\n",
        "        return map\n",
        "    return np.concatenate([map, np.array([pos_or_neg])])\n",
        "\n",
        "def read_data(filename, pos_or_neg):\n",
        "    file = open(filename, 'r')\n",
        "    lines=file.readlines()\n",
        "    data = np.zeros((len(lines), 181))\n",
        "    if pos_or_neg == None:\n",
        "        data = np.zeros((len(lines), 180))\n",
        "    for i, line in enumerate(lines):\n",
        "        input = data_to_input(line.replace('\\n', ''), pos_or_neg)\n",
        "        data[i] = input\n",
        "    return data\n",
        "\n",
        "# def bootstrap(DATA,size,NUMBER_OF_BATCHS):\n",
        "\n",
        "#     new_DATA=np.zeros((size,181))\n",
        "#     N=DATA.shape[0]\n",
        "#     batch_size=N//NUMBER_OF_BATCHS\n",
        "#     for i in range(NUMBER_OF_BATCHS):\n",
        "\n",
        "#         random = np.random.randint(batch_size*i,batch_size*(i+1), size=size//NUMBER_OF_BATCHS)\n",
        "#         new_DATA[((size//NUMBER_OF_BATCHS)*i):(size//NUMBER_OF_BATCHS)*(i+1), :] = DATA[random, :]\n",
        "#     return new_DATA\n",
        "\n",
        "\n",
        "def bootstrap(data,size,NUMBER_OF_BATCHS):\n",
        "\n",
        "    N=data.shape[0]\n",
        "    batch_size=N//NUMBER_OF_BATCHS\n",
        "    for i in range(NUMBER_OF_BATCHS):\n",
        "        random = np.random.randint(batch_size*i,batch_size*(i+1), size=size//NUMBER_OF_BATCHS)\n",
        "        data =np.vstack([DATA,DATA[random, :]])\n",
        "    return data\n",
        "\n",
        "\n",
        "def data_pre_pros(filename_pos,filename_neg):\n",
        "\n",
        "    neg_data=read_data(filename_neg, 0)\n",
        "    pos_data=read_data(filename_pos, 1)\n",
        "    \n",
        "    neg_data_train = neg_data[:int(len(neg_data)*0.9)]\n",
        "    neg_data_test = neg_data[int(len(neg_data)*0.9):]\n",
        "    pos_data_train = pos_data[:int(len(pos_data)*0.9)]\n",
        "    pos_data_test = pos_data[int(len(pos_data)*0.9):]\n",
        "    #pos_data_train = bootstrap(pos_data_train, int(BOOTSTRAP_SIZE*0.9), int(NUMBER_OF_BATCHS*0.9))\n",
        "    #pos_data_test = bootstrap(pos_data_test, int(BOOTSTRAP_SIZE*0.1), int(NUMBER_OF_BATCHS*0.1))\n",
        "\n",
        "    final_data_train = np.concatenate([neg_data_train, pos_data_train])\n",
        "    final_data_test = np.concatenate([neg_data_test, pos_data_test])\n",
        "    return final_data_train, final_data_test\n",
        "\n",
        "\n",
        "def shuffle_data(data_Xy):\n",
        "    np.random.shuffle(data_Xy)\n",
        "    return data_Xy[:,:180],data_Xy[:,-1]\n",
        "\n",
        "\n",
        "def spike_seq(filename):\n",
        "    with open(filename) as f:\n",
        "        lines = f.readlines()[0]\n",
        "        print(lines)\n",
        "        predeict=list()\n",
        "\n",
        "        if len(lines) == 9:\n",
        "            map = np.zeros((9, 20))\n",
        "            for i, seq in enumerate(lines):\n",
        "                map[i, MAPPING[seq]] += 1\n",
        "            map = map.flatten()\n",
        "            predeict.append(map)\n",
        "        else:\n",
        "            for i in range(len(lines)-9):\n",
        "                map = np.zeros((9, 20))\n",
        "                for i, seq in enumerate(lines[i:i+9]):\n",
        "                    map[i, MAPPING[seq]] += 1\n",
        "                map = map.flatten()\n",
        "                predeict.append(map)\n",
        "        \n",
        "        return np.array(predeict)\n"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYNbU_SaRgzZ"
      },
      "source": [
        "filename_pos='/content/ex1_DL/resorces/pos_A0201.txt'\n",
        "filename_neg='/content/ex1_DL/resorces/neg_A0201.txt'\n",
        "\n",
        "train_set, test_set=data_pre_pros(filename_pos,filename_neg)\n",
        "train_x, train_y = train_set[:,:180], train_set[:,-1]\n",
        "test_x, test_y = test_set[:,:180], test_set[:,-1]\n",
        "\n",
        "train_target = torch.from_numpy(train_y.astype(np.int64))\n",
        "train = torch.from_numpy(train_x.astype(np.float32)) \n",
        "train_tensor = torch.utils.data.TensorDataset(train, train_target) \n",
        "train_dataloader = torch.utils.data.DataLoader(dataset = train_tensor, batch_size = BATCH_SIZE, shuffle = True)\n",
        "\n",
        "test_target = torch.from_numpy(test_y.astype(np.int64))\n",
        "test = torch.from_numpy(test_x.astype(np.float32)) \n",
        "test_tensor = torch.utils.data.TensorDataset(test, test_target) \n",
        "test_dataloader = torch.utils.data.DataLoader(dataset = test_tensor, batch_size = BATCH_SIZE, shuffle = True)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkOKJ3pkTRut"
      },
      "source": [
        "# Evaluation functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08W0jQWQTnvo"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "# def calculate_accuracy(y_pred, y):\n",
        "#     top_pred = y_pred.argmax(1, keepdim = True)\n",
        "#     correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
        "#     acc = correct.float() / y.shape[0]\n",
        "#     return acc\n",
        "\n",
        "\n",
        "# def calculate_accuracy(y_pred, y):\n",
        "#     top_pred = y_pred.argmax(1, keepdim = True)\n",
        "#     f1 = f1_score(y_pred,y)\n",
        "#     return f1\n",
        "    \n",
        "\n",
        "def calculate_accuracy(y_true, y_pred, is_training=False):\n",
        "    if y_pred.ndim == 2:\n",
        "        y_pred = y_pred.argmax(dim=1)\n",
        "       \n",
        "   \n",
        "    tp = (y_true * y_pred).sum().to(torch.float32)\n",
        "    tn = ((1 - y_true) * (1 - y_pred)).sum().to(torch.float32)\n",
        "    fp = ((1 - y_true) * y_pred).sum().to(torch.float32)\n",
        "    fn = (y_true * (1 - y_pred)).sum().to(torch.float32)\n",
        "   \n",
        "    epsilon = 1e-7\n",
        "   \n",
        "    precision = tp / (tp + fp + epsilon)\n",
        "    recall = tp / (tp + fn + epsilon)\n",
        "    accuracy = (tp + tn) / (tp + tn + fp + fn + epsilon)\n",
        "   \n",
        "    f1 = 2* (precision*recall) / (precision + recall + epsilon)\n",
        "    f1.requires_grad = is_training\n",
        "    return accuracy"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhkcZrZYU9jQ"
      },
      "source": [
        "## Network architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zU3l7iJlVG6k"
      },
      "source": [
        "# Network parameters\n",
        "INPUT=180\n",
        "INPUT_1=15\n",
        "INPUT_2=10\n",
        "INPUT_3=4\n",
        "INPUT_4=32\n",
        "INPUT_5=16\n",
        "INPUT_6=8\n",
        "INPUT_7=4\n",
        "OUTPUT=2\n",
        "P_DROPOUT=0.1\n",
        "P_DROPOUT_2=0.15\n",
        "\n",
        "LEARNNING_RATE=0.000075\n",
        "\n",
        "BOOTSTRAP_SIZE=15000\n",
        "NUMBER_OF_BATCHS=150\n",
        "\n",
        "EPOCHS = 40"
      ],
      "execution_count": 368,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9TzVlOwVcaI"
      },
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Network, self).__init__()\n",
        "\n",
        "        self.input_fc = nn.Linear(INPUT, INPUT_1)\n",
        "        \n",
        "        self.hidden_1_fc = nn.Linear(INPUT_1, INPUT_2)\n",
        "        \n",
        "        self.hidden_2_fc = nn.Linear(INPUT_2, INPUT_3)\n",
        "\n",
        "        self.output_fc = nn.Linear(INPUT_3, OUTPUT)\n",
        "\n",
        "        self.dropout = nn.Dropout(P_DROPOUT)\n",
        "\n",
        "        \n",
        "        self.batch_norm_1 = nn.BatchNorm1d(INPUT_1)\n",
        "        self.batch_norm_2 = nn.BatchNorm1d(INPUT_2)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "   \n",
        "\n",
        "        h_1 = F.relu(self.batch_norm_1(self.input_fc(x)))\n",
        "\n",
        "        h_1=self.dropout(h_1)\n",
        "\n",
        "        h_2 = F.relu(self.batch_norm_2(self.hidden_1_fc(h_1)))\n",
        "\n",
        "        # h_2=self.dropout(h_2)\n",
        "\n",
        "        h_3 = F.relu(self.hidden_2_fc(h_2))\n",
        "\n",
        "        #h_3=self.dropout2(h_3)\n",
        "\n",
        "        y_pred = self.output_fc(h_3)\n",
        "\n",
        "        return y_pred, h_3\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Network().to(device)\n",
        "optimizer=optim.Adam(model.parameters(), lr=LEARNNING_RATE)\n",
        "weights = [train_set.shape[0]/(2*2.4*np.sum(train_set[:, -1])), 2.4*np.sum(train_set[:, -1])/(2*(train_set.shape[0]-np.sum(train_set[:, -1])))] #as class distribution\n",
        "weights.reverse()\n",
        "class_weights = torch.FloatTensor(weights).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)"
      ],
      "execution_count": 369,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mb8kMOWPiG_"
      },
      "source": [
        "# Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uFUj0MVPgoO",
        "outputId": "fc93fc54-99ef-4986-ddca-b67d928b26a4"
      },
      "source": [
        "start_time=time()\n",
        "\n",
        "for epoch in range(20):\n",
        "    running_loss=0.0\n",
        "    pbar=tqdm(iterable=train_dataloader)\n",
        "    for i,batch in enumerate(pbar):\n",
        "      batch=[item.to(device) for  item in batch]\n",
        "      sequences, labels=batch\n",
        "      # zero the parameter gradients\n",
        "      optimizer.zero_grad()\n",
        "      # forward \n",
        "      outputs, _ = model(sequences)\n",
        "      #calculate the loss between the target and the actuals\n",
        "      loss= criterion(input=outputs, target=labels)\n",
        "      #Gradient calculation uisng backward pass\n",
        "      loss.backward()\n",
        "      # update the weights\n",
        "      optimizer.step()\n",
        "      running_loss+=loss.item()\n",
        "      pbar.set_postfix(loss=running_loss/(i+1))\n",
        "    pbar.close()\n",
        "    print('epoch %d -Loss %.3f' % (epoch +1,running_loss/(len(train_tensor)/BATCH_SIZE)))      \n",
        "    running_loss = 0.0\n",
        "\n",
        "print(\"Time for training using PyTorch %f\" %(time()-start_time))"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 387/387 [00:03<00:00, 122.14it/s, loss=0.289]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1 -Loss 0.289\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 387/387 [00:03<00:00, 121.26it/s, loss=0.131]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2 -Loss 0.131\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 387/387 [00:03<00:00, 122.79it/s, loss=0.12]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3 -Loss 0.120\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 387/387 [00:03<00:00, 121.10it/s, loss=0.116]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 4 -Loss 0.116\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 387/387 [00:03<00:00, 121.53it/s, loss=0.114]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 5 -Loss 0.114\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 387/387 [00:03<00:00, 122.54it/s, loss=0.116]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 6 -Loss 0.116\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 387/387 [00:03<00:00, 122.28it/s, loss=0.11]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 7 -Loss 0.110\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 387/387 [00:03<00:00, 120.90it/s, loss=0.114]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 8 -Loss 0.114\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 387/387 [00:03<00:00, 120.51it/s, loss=0.109]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 9 -Loss 0.109\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 387/387 [00:03<00:00, 118.58it/s, loss=0.109]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 10 -Loss 0.110\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 387/387 [00:03<00:00, 117.21it/s, loss=0.111]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 11 -Loss 0.111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 387/387 [00:03<00:00, 119.94it/s, loss=0.11]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 12 -Loss 0.110\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 387/387 [00:03<00:00, 120.39it/s, loss=0.107]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 13 -Loss 0.107\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 387/387 [00:03<00:00, 120.04it/s, loss=0.109]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 14 -Loss 0.109\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 387/387 [00:03<00:00, 120.90it/s, loss=0.108]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 15 -Loss 0.108\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 387/387 [00:03<00:00, 119.06it/s, loss=0.106]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 16 -Loss 0.106\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 387/387 [00:03<00:00, 118.97it/s, loss=0.106]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 17 -Loss 0.106\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 387/387 [00:03<00:00, 117.70it/s, loss=0.108]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 18 -Loss 0.108\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 387/387 [00:03<00:00, 118.98it/s, loss=0.105]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 19 -Loss 0.105\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 387/387 [00:03<00:00, 120.47it/s, loss=0.106]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 20 -Loss 0.106\n",
            "Time for training using PyTorch 64.577247\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34cbmGnjpMtR"
      },
      "source": [
        "def train_model(model, train_dataloader, optimizer, criterion, device):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.train()\n",
        "\n",
        "    for i,batch in enumerate(train_dataloader):\n",
        "        batch=[item.to(device) for item in batch]\n",
        "        sequences, labels=batch\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "      \n",
        "        # forward \n",
        "        outputs, _ = model(sequences)\n",
        "        #calculate the loss between the target and the actuals\n",
        "        loss= criterion(input=outputs, target=labels)\n",
        "        #Gradient calculation uisng backward pass\n",
        "        acc=calculate_accuracy(labels, outputs)\n",
        "        loss.backward()\n",
        "        # update the weights\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(train_dataloader), epoch_acc / len(train_dataloader)\n",
        "\n",
        "\n",
        "def evaluate(model, test_dataloader, criterion, device):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.eval()\n",
        "\n",
        "    for i,batch in enumerate(test_dataloader):\n",
        "        batch=[item.to(device) for item in batch]\n",
        "        sequences, labels=batch\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "      \n",
        "        # forward \n",
        "        outputs, _ = model(sequences)\n",
        "        #calculate the loss between the target and the actuals\n",
        "        loss= criterion(input=outputs, target=labels)\n",
        "        #Gradient calculation uisng backward pass\n",
        "        acc=calculate_accuracy(labels, outputs)\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(test_dataloader), epoch_acc / len(test_dataloader)\n"
      ],
      "execution_count": 364,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "D7OQ_DBFrHs7",
        "outputId": "cafd0874-5bb5-42ee-b340-435ce6cf06b3"
      },
      "source": [
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "print(f'weights: {weights}')\n",
        "train_per_ep=[]\n",
        "test_per_ep=[]\n",
        "test_acc = 0\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss, train_acc = train_model(model, train_dataloader, optimizer, criterion, device)\n",
        "    test_loss, test_acc = evaluate(model, test_dataloader,criterion, device)\n",
        "    \n",
        "    train_per_ep.append(train_loss)\n",
        "    test_per_ep.append(test_loss)\n",
        "    \n",
        "    print(f'Epoch: {epoch + 1:02}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')\n",
        "    print(f'Epoch: {epoch + 1:02}')\n",
        "    print(f'\\tTest Loss: {test_loss:.3f} | Test Acc: {test_acc * 100:.2f}%')\n",
        "\n",
        "train_per_ep=np.array(train_per_ep)\n",
        "test_per_ep=np.array(test_per_ep)\n",
        "epocs=np.arange(1,EPOCHS+1)\n",
        "print(np.mean(np.abs(np.array(train_per_ep) - np.array(test_per_ep))))\n",
        "plt.figure()\n",
        "plt.plot(epocs,train_per_ep)\n",
        "plt.plot(epocs,test_per_ep)\n",
        "plt.legend(['train loss','test loss'])\n",
        "plt.show()"
      ],
      "execution_count": 370,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 2,979 trainable parameters\n",
            "weights: [0.14650213229289538, 1.9147931376192246]\n",
            "Epoch: 01\n",
            "\tTrain Loss: 0.674 | Train Acc: 20.58%\n",
            "Epoch: 01\n",
            "\tTest Loss: 0.655 | Test Acc: 20.98%\n",
            "Epoch: 02\n",
            "\tTrain Loss: 0.633 | Train Acc: 26.66%\n",
            "Epoch: 02\n",
            "\tTest Loss: 0.607 | Test Acc: 32.18%\n",
            "Epoch: 03\n",
            "\tTrain Loss: 0.578 | Train Acc: 39.17%\n",
            "Epoch: 03\n",
            "\tTest Loss: 0.541 | Test Acc: 45.96%\n",
            "Epoch: 04\n",
            "\tTrain Loss: 0.510 | Train Acc: 52.92%\n",
            "Epoch: 04\n",
            "\tTest Loss: 0.467 | Test Acc: 61.93%\n",
            "Epoch: 05\n",
            "\tTrain Loss: 0.448 | Train Acc: 63.96%\n",
            "Epoch: 05\n",
            "\tTest Loss: 0.406 | Test Acc: 68.43%\n",
            "Epoch: 06\n",
            "\tTrain Loss: 0.392 | Train Acc: 71.26%\n",
            "Epoch: 06\n",
            "\tTest Loss: 0.361 | Test Acc: 74.04%\n",
            "Epoch: 07\n",
            "\tTrain Loss: 0.348 | Train Acc: 75.23%\n",
            "Epoch: 07\n",
            "\tTest Loss: 0.320 | Test Acc: 77.74%\n",
            "Epoch: 08\n",
            "\tTrain Loss: 0.321 | Train Acc: 77.85%\n",
            "Epoch: 08\n",
            "\tTest Loss: 0.297 | Test Acc: 79.28%\n",
            "Epoch: 09\n",
            "\tTrain Loss: 0.301 | Train Acc: 79.76%\n",
            "Epoch: 09\n",
            "\tTest Loss: 0.279 | Test Acc: 81.02%\n",
            "Epoch: 10\n",
            "\tTrain Loss: 0.288 | Train Acc: 80.54%\n",
            "Epoch: 10\n",
            "\tTest Loss: 0.268 | Test Acc: 82.26%\n",
            "Epoch: 11\n",
            "\tTrain Loss: 0.277 | Train Acc: 81.70%\n",
            "Epoch: 11\n",
            "\tTest Loss: 0.261 | Test Acc: 82.59%\n",
            "Epoch: 12\n",
            "\tTrain Loss: 0.268 | Train Acc: 82.16%\n",
            "Epoch: 12\n",
            "\tTest Loss: 0.258 | Test Acc: 81.31%\n",
            "Epoch: 13\n",
            "\tTrain Loss: 0.264 | Train Acc: 82.58%\n",
            "Epoch: 13\n",
            "\tTest Loss: 0.248 | Test Acc: 83.12%\n",
            "Epoch: 14\n",
            "\tTrain Loss: 0.255 | Train Acc: 83.17%\n",
            "Epoch: 14\n",
            "\tTest Loss: 0.247 | Test Acc: 82.91%\n",
            "Epoch: 15\n",
            "\tTrain Loss: 0.250 | Train Acc: 83.65%\n",
            "Epoch: 15\n",
            "\tTest Loss: 0.242 | Test Acc: 83.86%\n",
            "Epoch: 16\n",
            "\tTrain Loss: 0.246 | Train Acc: 83.82%\n",
            "Epoch: 16\n",
            "\tTest Loss: 0.234 | Test Acc: 84.41%\n",
            "Epoch: 17\n",
            "\tTrain Loss: 0.243 | Train Acc: 84.23%\n",
            "Epoch: 17\n",
            "\tTest Loss: 0.236 | Test Acc: 84.07%\n",
            "Epoch: 18\n",
            "\tTrain Loss: 0.242 | Train Acc: 84.43%\n",
            "Epoch: 18\n",
            "\tTest Loss: 0.240 | Test Acc: 83.71%\n",
            "Epoch: 19\n",
            "\tTrain Loss: 0.242 | Train Acc: 84.35%\n",
            "Epoch: 19\n",
            "\tTest Loss: 0.236 | Test Acc: 83.86%\n",
            "Epoch: 20\n",
            "\tTrain Loss: 0.236 | Train Acc: 84.77%\n",
            "Epoch: 20\n",
            "\tTest Loss: 0.242 | Test Acc: 84.00%\n",
            "Epoch: 21\n",
            "\tTrain Loss: 0.231 | Train Acc: 85.03%\n",
            "Epoch: 21\n",
            "\tTest Loss: 0.236 | Test Acc: 84.58%\n",
            "Epoch: 22\n",
            "\tTrain Loss: 0.236 | Train Acc: 85.07%\n",
            "Epoch: 22\n",
            "\tTest Loss: 0.237 | Test Acc: 84.98%\n",
            "Epoch: 23\n",
            "\tTrain Loss: 0.230 | Train Acc: 85.01%\n",
            "Epoch: 23\n",
            "\tTest Loss: 0.250 | Test Acc: 84.80%\n",
            "Epoch: 24\n",
            "\tTrain Loss: 0.229 | Train Acc: 85.10%\n",
            "Epoch: 24\n",
            "\tTest Loss: 0.244 | Test Acc: 84.94%\n",
            "Epoch: 25\n",
            "\tTrain Loss: 0.226 | Train Acc: 85.57%\n",
            "Epoch: 25\n",
            "\tTest Loss: 0.237 | Test Acc: 84.25%\n",
            "Epoch: 26\n",
            "\tTrain Loss: 0.222 | Train Acc: 85.61%\n",
            "Epoch: 26\n",
            "\tTest Loss: 0.236 | Test Acc: 86.44%\n",
            "Epoch: 27\n",
            "\tTrain Loss: 0.225 | Train Acc: 85.84%\n",
            "Epoch: 27\n",
            "\tTest Loss: 0.239 | Test Acc: 86.40%\n",
            "Epoch: 28\n",
            "\tTrain Loss: 0.217 | Train Acc: 86.12%\n",
            "Epoch: 28\n",
            "\tTest Loss: 0.230 | Test Acc: 86.00%\n",
            "Epoch: 29\n",
            "\tTrain Loss: 0.216 | Train Acc: 86.39%\n",
            "Epoch: 29\n",
            "\tTest Loss: 0.238 | Test Acc: 85.56%\n",
            "Epoch: 30\n",
            "\tTrain Loss: 0.216 | Train Acc: 86.35%\n",
            "Epoch: 30\n",
            "\tTest Loss: 0.231 | Test Acc: 86.36%\n",
            "Epoch: 31\n",
            "\tTrain Loss: 0.215 | Train Acc: 86.31%\n",
            "Epoch: 31\n",
            "\tTest Loss: 0.239 | Test Acc: 87.02%\n",
            "Epoch: 32\n",
            "\tTrain Loss: 0.211 | Train Acc: 86.56%\n",
            "Epoch: 32\n",
            "\tTest Loss: 0.245 | Test Acc: 86.04%\n",
            "Epoch: 33\n",
            "\tTrain Loss: 0.210 | Train Acc: 86.60%\n",
            "Epoch: 33\n",
            "\tTest Loss: 0.239 | Test Acc: 85.96%\n",
            "Epoch: 34\n",
            "\tTrain Loss: 0.212 | Train Acc: 86.67%\n",
            "Epoch: 34\n",
            "\tTest Loss: 0.243 | Test Acc: 85.45%\n",
            "Epoch: 35\n",
            "\tTrain Loss: 0.210 | Train Acc: 86.71%\n",
            "Epoch: 35\n",
            "\tTest Loss: 0.234 | Test Acc: 86.43%\n",
            "Epoch: 36\n",
            "\tTrain Loss: 0.203 | Train Acc: 87.02%\n",
            "Epoch: 36\n",
            "\tTest Loss: 0.244 | Test Acc: 86.47%\n",
            "Epoch: 37\n",
            "\tTrain Loss: 0.201 | Train Acc: 87.37%\n",
            "Epoch: 37\n",
            "\tTest Loss: 0.243 | Test Acc: 86.76%\n",
            "Epoch: 38\n",
            "\tTrain Loss: 0.203 | Train Acc: 87.41%\n",
            "Epoch: 38\n",
            "\tTest Loss: 0.243 | Test Acc: 86.98%\n",
            "Epoch: 39\n",
            "\tTrain Loss: 0.202 | Train Acc: 87.19%\n",
            "Epoch: 39\n",
            "\tTest Loss: 0.238 | Test Acc: 86.73%\n",
            "Epoch: 40\n",
            "\tTrain Loss: 0.201 | Train Acc: 87.45%\n",
            "Epoch: 40\n",
            "\tTest Loss: 0.253 | Test Acc: 87.82%\n",
            "0.02166989414305342\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lw7GZ0uXab_W",
        "outputId": "d8d3ee0a-c4b5-4f3a-a728-326db4335223",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "seq=spike_seq('/content/ex1_DL/resorces/spike_acid.txt')\n",
        "print(len(seq))\n",
        "seq = torch.from_numpy(seq.astype('float32'))\n",
        "seq = seq.to(device)\n",
        "y_pred, _ = model(seq)\n",
        "z = y_pred\n",
        "my_softmax = nn.Softmax(dim=1)\n",
        "z = my_softmax(z)\n",
        "# print(len(torch.where(z[:,1] > 0.5)[0]) / len(z))\n",
        "# print(len(torch.where(z[:,0] > 0.5)[0]) / len(z))\n",
        "print(len(torch.where(z[:,1] > 0.96)[0]))\n",
        "\n"
      ],
      "execution_count": 290,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSSVLHSTQDLFLPFFSNVTWFHAIHVSGTNGTKRFDNPVLPFNDGVYFASTEKSNIIRGWIFGTTLDSKTQSLLIVNNATNVVIKVCEFQFCNDPFLGVYYHKNNKSWMESEFRVYSSANNCTFEYVSQPFLMDLEGKQGNFKNLREFVFKNIDGYFKIYSKHTPINLVRDLPQGFSALEPLVDLPIGINITRFQTLLALHRSYLTPGDSSSGWTAGAAAYYVGYLQPRTFLLKYNENGTITDAVDCALDPLSETKCTLKSFTVEKGIYQTSNFRVQPTESIVRFPNITNLCPFGEVFNATRFASVYAWNRKRISNCVADYSVLYNSASFSTFKCYGVSPTKLNDLCFTNVYADSFVIRGDEVRQIAPGQTGKIADYNYKLPDDFTGCVIAWNSNNLDSKVGGNYNYLYRLFRKSNLKPFERDISTEIYQAGSTPCNGVEGFNCYFPLQSYGFQPTNGVGYQPYRVVVLSFELLHAPATVCGPKKSTNLVKNKCVNFNFNGLTGTGVLTESNKKFLPFQQFGRDIADTTDAVRDPQTLEILDITPCSFGGVSVITPGTNTSNQVAVLYQDVNCTEVPVAIHADQLTPTWRVYSTGSNVFQTRAGCLIGAEHVNNSYECDIPIGAGICASYQTQTNSPRRARSVASQSIIAYTMSLGAENSVAYSNNSIAIPTNFTISVTTEILPVSMTKTSVDCTMYICGDSTECSNLLLQYGSFCTQLNRALTGIAVEQDKNTQEVFAQVKQIYKTPPIKDFGGFNFSQILPDPSKPSKRSFIEDLLFNKVTLADAGFIKQYGDCLGDIAARDLICAQKFNGLTVLPPLLTDEMIAQYTSALLAGTITSGWTFGAGAALQIPFAMQMAYRFNGIGVTQNVLYENQKLIANQFNSAIGKIQDSLSSTASALGKLQDVVNQNAQALNTLVKQLSSNFGAISSVLNDILSRLDKVEAEVQIDRLITGRLQSLQTYVTQQLIRAAEIRASANLAATKMSECVLGQSKRVDFCGKGYHLMSFPQSAPHGVVFLHVTYVPAQEKNFTTAPAICHDGKAHFPREGVFVSNGTHWFVTQRNFYEPQIITTDNTFVSGNCDVVIGIVNNTVYDPLQPELDSFKEELDKYFKNHTSPDVDLGDISGINASVVNIQKEIDRLNEVAKNLNESLIDLQELGKYEQYIKWPWYIWLGFIAGLIAIVMVTIMLCCMTSCCSCLKGCCSCGSCCKFDEDDSEPVLKGVKLHYT\n",
            "1264\n",
            "5\n"
          ]
        }
      ]
    }
  ]
}