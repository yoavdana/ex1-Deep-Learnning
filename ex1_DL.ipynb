{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ex1_DL.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "17Izj8qnhuW6mjGMiaR02Z_OcQJ0eGk-V",
      "authorship_tag": "ABX9TyN+q8UWHkJj1ItzXvjNk2sg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoavdana/ex1_DL/blob/main/ex1_DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tNlOLLOv5q8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2f70484-aed8-4f71-c365-850d238e3933"
      },
      "source": [
        "!git clone https://github.com/yoavdana/ex1_DL.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ex1_DL'...\n",
            "remote: Enumerating objects: 44, done.\u001b[K\n",
            "remote: Counting objects: 100% (44/44), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 44 (delta 11), reused 30 (delta 7), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (44/44), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9c1XQ2OEx8t"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import scipy.stats as si\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import time\n",
        "\n",
        "INPUT_DIM = 9 * 20\n",
        "OUTPUT_DIM = 2\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5V0o627FZyo"
      },
      "source": [
        "SEQ_LENGTH=20\n",
        "BOOTSTRAP_SIZE=10000\n",
        "NUMBER_OF_BATCHS=4\n",
        "\n",
        "\n",
        "\n",
        "def data_to_input(sequence, pos_or_neg):\n",
        "    mapping = {'A': 0, 'R': 1, 'N': 2, 'D': 3, 'C': 4, 'E': 5, 'Q': 6, 'G': 7, 'H': 8, 'I': 9, 'L': 10, 'K': 11,\n",
        "               'M': 12, 'F': 13, 'P': 14, 'S': 15, 'T': 16, 'W': 17, 'Y': 18, 'V': 19}\n",
        "    map=np.zeros((9, 20))\n",
        "    for i, seq in enumerate(sequence):\n",
        "        map[i,mapping[seq]]+=1\n",
        "    map = map.flatten()\n",
        "    return np.concatenate([map, np.array([pos_or_neg])])\n",
        "\n",
        "\n",
        "def Read_Data(filename, pos_or_neg):\n",
        "    file = open(filename, 'r')\n",
        "    lines=file.readlines()\n",
        "    if pos_or_neg==1:\n",
        "        DATA=np.zeros((len(lines), 181))\n",
        "        for i, line in enumerate(lines):\n",
        "            input = data_to_input(line.replace('\\n', ''), pos_or_neg)\n",
        "            DATA[i] = input\n",
        "        DATA=bootstrap(DATA, BOOTSTRAP_SIZE, NUMBER_OF_BATCHS)\n",
        "    else:\n",
        "        DATA = np.zeros((len(lines), 181))\n",
        "        for i, line in enumerate(lines):\n",
        "            input = data_to_input(line.replace('\\n', ''), pos_or_neg)\n",
        "            DATA[i] = input\n",
        "    return DATA\n",
        "\n",
        "\n",
        "def bootstrap(DATA,size,NUMBER_OF_BATCHS):\n",
        "\n",
        "    new_DATA=np.zeros((size,181))\n",
        "    N=DATA.shape[0]\n",
        "    batch_size=N//NUMBER_OF_BATCHS\n",
        "    for i in range(NUMBER_OF_BATCHS):\n",
        "\n",
        "        random = np.random.randint(batch_size*i,batch_size*(i+1), size=size//NUMBER_OF_BATCHS)\n",
        "        new_DATA[((size//NUMBER_OF_BATCHS)*i):(size//NUMBER_OF_BATCHS)*(i+1), :] = DATA[random, :]\n",
        "    return new_DATA\n",
        "\n",
        "def DATA_pre_pros(filename_pos,filename_neg):\n",
        "\n",
        "    neg_data=Read_Data(filename_neg, 0)\n",
        "    pos_data=Read_Data(filename_pos, 1)\n",
        "    final_data = np.concatenate([neg_data, pos_data])\n",
        "    np.random.shuffle(final_data)\n",
        "    train_set = final_data[:int(len(final_data)*0.9)]\n",
        "    test_set = final_data[int(len(final_data)*0.9):]\n",
        "    return train_set, test_set\n",
        "\n",
        "\n",
        "\n",
        "def shuffle_data(data_Xy):\n",
        "    np.random.shuffle(data_Xy)\n",
        "    return data_Xy[:,:180],data_Xy[:,-1]\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HutCXePsE9KO"
      },
      "source": [
        "\n",
        "class NetWork(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_fc = nn.Linear(input_dim, 60)\n",
        "        self.hidden_fc = nn.Linear(60, 25)\n",
        "        self.output_fc = nn.Linear(25, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = [batch size, height, width]\n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        # x = x.view(batch_size, -1)\n",
        "\n",
        "        # x = [batch size, height * width]\n",
        "\n",
        "        h_1 = F.relu(self.input_fc(x))\n",
        "\n",
        "        # h_1 = [batch size, 250]\n",
        "\n",
        "        h_2 = F.relu(self.hidden_fc(h_1))\n",
        "\n",
        "        # h_2 = [batch size, 100]\n",
        "\n",
        "        y_pred = self.output_fc(h_2)\n",
        "\n",
        "        # y_pred = [batch size, output dim]\n",
        "        \n",
        "        \n",
        "        return y_pred, h_2\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1jysnYBFurA"
      },
      "source": [
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def calculate_accuracy(y_pred, y):\n",
        "    top_pred = y_pred.argmax(1, keepdim = True)\n",
        "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
        "    acc = correct.float() / y.shape[0]\n",
        "    return acc\n",
        "\n",
        "\n",
        "def train(model, iterator_x, iterator_y, optimizer, criterion, device):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for i, x in enumerate(iterator_x):\n",
        "        x = x.to(device)\n",
        "        y = iterator_y[i].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        y_pred, _ = model(x)\n",
        "\n",
        "        loss = criterion(y_pred, y)\n",
        "\n",
        "        acc = calculate_accuracy(y_pred, y)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator_x), epoch_acc / len(iterator_x)\n",
        "\n",
        "\n",
        "def evaluate(model, iterator_x, iterator_y, criterion, device):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for i, x in enumerate(iterator_x):\n",
        "        x = x.to(device)\n",
        "        y = iterator_y[i].to(device)\n",
        "\n",
        "        y_pred, _ = model(x)\n",
        "\n",
        "        loss = criterion(y_pred, y)\n",
        "\n",
        "        acc = calculate_accuracy(y_pred, y)\n",
        "\n",
        " \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator_x), epoch_acc / len(iterator_x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNHufHg2Gyz5",
        "outputId": "5eb80c2d-23c1-4f03-fe36-50d201e3405c"
      },
      "source": [
        "filename_pos='/content/ex1_DL/resorces/pos_A0201.txt'\n",
        "filename_neg='/content/ex1_DL/resorces/neg_A0201.txt'\n",
        "\n",
        "train_set, test_set=DATA_pre_pros(filename_pos,filename_neg)\n",
        "\n",
        "\n",
        "\n",
        "model = NetWork(INPUT_DIM, OUTPUT_DIM)\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "\n",
        "# BATCH_SIZE = 64\n",
        "# batch_x=train_x[0:BATCH_SIZE]\n",
        "# batch_y=train_y[0:BATCH_SIZE]\n",
        "# batch_x = torch.from_numpy(batch_x.astype('float32'))\n",
        "# batch_y = torch.from_numpy(batch_y.astype('int64'))\n",
        "# model.train()\n",
        "# y_p=model(batch_x)\n",
        "# print(y_p)\n",
        "\n",
        "\n",
        "EPOCHS = 10\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "    train_x,train_y=shuffle_data(train_set)\n",
        "    train_iterator_x = torch.from_numpy(train_x.astype('float32')).split(64)\n",
        "    train_iterator_y = torch.from_numpy(train_y.astype('int64')).split(64)\n",
        "\n",
        "    test_x,test_y=shuffle_data(test_set)\n",
        "    test_iterator_x = torch.from_numpy(test_x.astype('float32')).split(64)\n",
        "    test_iterator_y = torch.from_numpy(test_y.astype('int64')).split(64)\n",
        "\n",
        "    train_loss, train_acc = train(model, train_iterator_x, train_iterator_y, optimizer, criterion, device)\n",
        "\n",
        "    test_loss, test_acc = evaluate(model, test_iterator_x, test_iterator_y,criterion, device)\n",
        "    \n",
        "    \n",
        "    print(f'Epoch: {epoch + 1:02}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')\n",
        "    print(f'Epoch: {epoch + 1:02}')\n",
        "    print(f'\\tTest Loss: {test_loss:.3f} | Test Acc: {test_acc * 100:.2f}%')\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 12,437 trainable parameters\n",
            "Epoch: 01\n",
            "\tTrain Loss: 0.295 | Train Acc: 86.60%\n",
            "Epoch: 01\n",
            "\tTest Loss: 0.235 | Test Acc: 89.56%\n",
            "Epoch: 02\n",
            "\tTrain Loss: 0.220 | Train Acc: 90.57%\n",
            "Epoch: 02\n",
            "\tTest Loss: 0.218 | Test Acc: 90.44%\n",
            "Epoch: 03\n",
            "\tTrain Loss: 0.197 | Train Acc: 91.88%\n",
            "Epoch: 03\n",
            "\tTest Loss: 0.197 | Test Acc: 91.65%\n",
            "Epoch: 04\n",
            "\tTrain Loss: 0.170 | Train Acc: 93.24%\n",
            "Epoch: 04\n",
            "\tTest Loss: 0.181 | Test Acc: 92.67%\n",
            "Epoch: 05\n",
            "\tTrain Loss: 0.145 | Train Acc: 94.61%\n",
            "Epoch: 05\n",
            "\tTest Loss: 0.172 | Test Acc: 93.14%\n",
            "Epoch: 06\n",
            "\tTrain Loss: 0.124 | Train Acc: 95.49%\n",
            "Epoch: 06\n",
            "\tTest Loss: 0.162 | Test Acc: 93.79%\n",
            "Epoch: 07\n",
            "\tTrain Loss: 0.108 | Train Acc: 96.28%\n",
            "Epoch: 07\n",
            "\tTest Loss: 0.163 | Test Acc: 94.03%\n",
            "Epoch: 08\n",
            "\tTrain Loss: 0.093 | Train Acc: 96.77%\n",
            "Epoch: 08\n",
            "\tTest Loss: 0.155 | Test Acc: 94.41%\n",
            "Epoch: 09\n",
            "\tTrain Loss: 0.080 | Train Acc: 97.31%\n",
            "Epoch: 09\n",
            "\tTest Loss: 0.155 | Test Acc: 94.70%\n",
            "Epoch: 10\n",
            "\tTrain Loss: 0.070 | Train Acc: 97.76%\n",
            "Epoch: 10\n",
            "\tTest Loss: 0.157 | Test Acc: 94.81%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkTUKuGfTuIE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}