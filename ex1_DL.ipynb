{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ex1_DL.ipynb",
      "provenance": [],
      "mount_file_id": "https://github.com/yoavdana/ex1_DL/blob/main/ex1_DL.ipynb",
      "authorship_tag": "ABX9TyPfzC8f6w9/iXnJhXRSEgHU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoavdana/ex1_DL/blob/main/ex1_DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tNlOLLOv5q8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c23f2ef0-c88b-4230-a4ba-0245d0c6f8c5"
      },
      "source": [
        "!git clone https://github.com/yoavdana/ex1_DL.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'ex1_DL' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9c1XQ2OEx8t"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import scipy.stats as si\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "INPUT_DIM = 9 * 20\n",
        "OUTPUT_DIM = 2\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5V0o627FZyo"
      },
      "source": [
        "SEQ_LENGTH=20\n",
        "BOOTSTRAP_SIZE=24000\n",
        "NUMBER_OF_BATCHS=5\n",
        "\n",
        "\n",
        "\n",
        "def data_to_input(sequence, pos_or_neg):\n",
        "    mapping = {'A': 0, 'R': 1, 'N': 2, 'D': 3, 'C': 4, 'E': 5, 'Q': 6, 'G': 7, 'H': 8, 'I': 9, 'L': 10, 'K': 11,\n",
        "               'M': 12, 'F': 13, 'P': 14, 'S': 15, 'T': 16, 'W': 17, 'Y': 18, 'V': 19}\n",
        "    map=np.zeros((9, 20))\n",
        "    for i, seq in enumerate(sequence):\n",
        "        map[i,mapping[seq]]+=1\n",
        "    map = map.flatten()\n",
        "    return np.concatenate([map, np.array([pos_or_neg])])\n",
        "\n",
        "\n",
        "def Read_Data(filename, pos_or_neg):\n",
        "    file = open(filename, 'r')\n",
        "    lines=file.readlines()\n",
        "    if pos_or_neg==1:\n",
        "        DATA=np.zeros((len(lines), 181))\n",
        "        for i, line in enumerate(lines):\n",
        "            input = data_to_input(line.replace('\\n', ''), pos_or_neg)\n",
        "            DATA[i] = input\n",
        "        DATA=bootstrap(DATA, BOOTSTRAP_SIZE, NUMBER_OF_BATCHS)\n",
        "    else:\n",
        "        DATA = np.zeros((len(lines), 181))\n",
        "        for i, line in enumerate(lines):\n",
        "            input = data_to_input(line.replace('\\n', ''), pos_or_neg)\n",
        "            DATA[i] = input\n",
        "    return DATA\n",
        "\n",
        "\n",
        "def bootstrap(DATA,size,NUMBER_OF_BATCHS):\n",
        "\n",
        "    new_DATA=np.zeros((size,181))\n",
        "    N=DATA.shape[0]\n",
        "    batch_size=N//NUMBER_OF_BATCHS\n",
        "    for i in range(NUMBER_OF_BATCHS):\n",
        "\n",
        "        random = np.random.randint(batch_size*i,batch_size*(i+1), size=size//NUMBER_OF_BATCHS)\n",
        "        new_DATA[((size//NUMBER_OF_BATCHS)*i):(size//NUMBER_OF_BATCHS)*(i+1), :] = DATA[random, :]\n",
        "    return new_DATA\n",
        "\n",
        "\n",
        "def DATA_pre_pros(filename_pos,filename_neg):\n",
        "\n",
        "    neg_data=Read_Data(filename_neg, 0)\n",
        "    pos_data=Read_Data(filename_pos, 1)\n",
        "    final_data = np.concatenate([neg_data, pos_data])\n",
        "    np.random.shuffle(final_data)\n",
        "    train_set = final_data[:int(len(final_data)*0.9)]\n",
        "    test_set = final_data[int(len(final_data)*0.9):]\n",
        "    return train_set, test_set\n",
        "\n",
        "\n",
        "\n",
        "def shuffle_data(data_Xy):\n",
        "    np.random.shuffle(data_Xy)\n",
        "    return data_Xy[:,:180],data_Xy[:,-1]\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HutCXePsE9KO"
      },
      "source": [
        "INPUT_1=100\n",
        "INPUT_2=50\n",
        "INPUT_3=10\n",
        "P_DROPOUT=0.25\n",
        "\n",
        "\n",
        "class NetWork(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_fc = nn.Linear(input_dim, INPUT_1)\n",
        "        \n",
        "        self.hidden_1_fc = nn.Linear(INPUT_1, INPUT_2)\n",
        "        \n",
        "        self.hidden_2_fc = nn.Linear(INPUT_2, INPUT_3)\n",
        "        \n",
        "        self.output_fc = nn.Linear(INPUT_3, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(P_DROPOUT)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "   \n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "\n",
        "        h_1 = F.relu(self.input_fc(x))\n",
        "\n",
        "        h_1=self.dropout(h_1)\n",
        "       \n",
        "\n",
        "        h_2 = F.relu(self.hidden_1_fc(h_1))\n",
        "\n",
        "\n",
        "        h_3 = F.relu(self.hidden_2_fc(h_2))\n",
        "\n",
        "\n",
        "        y_pred = self.output_fc(h_3)\n",
        "\n",
        "       \n",
        "        \n",
        "        return y_pred, h_2\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1jysnYBFurA"
      },
      "source": [
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def calculate_accuracy(y_pred, y):\n",
        "    top_pred = y_pred.argmax(1, keepdim = True)\n",
        "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
        "    acc = correct.float() / y.shape[0]\n",
        "    return acc\n",
        "\n",
        "\n",
        "def train(model, iterator_x, iterator_y, optimizer, criterion, device):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for i, x in enumerate(iterator_x):\n",
        "        x = x.to(device)\n",
        "        y = iterator_y[i].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        y_pred, _ = model(x)\n",
        "\n",
        "        loss = criterion(y_pred, y)\n",
        "\n",
        "        acc = calculate_accuracy(y_pred, y)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator_x), epoch_acc / len(iterator_x)\n",
        "\n",
        "\n",
        "def evaluate(model, iterator_x, iterator_y, criterion, device):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for i, x in enumerate(iterator_x):\n",
        "        x = x.to(device)\n",
        "        y = iterator_y[i].to(device)\n",
        "\n",
        "        y_pred, _ = model(x)\n",
        "\n",
        "        loss = criterion(y_pred, y)\n",
        "\n",
        "        acc = calculate_accuracy(y_pred, y)\n",
        "\n",
        " \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator_x), epoch_acc / len(iterator_x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNHufHg2Gyz5"
      },
      "source": [
        "filename_pos='/content/ex1_DL/resorces/pos_A0201.txt'\n",
        "filename_neg='/content/ex1_DL/resorces/neg_A0201.txt'\n",
        "\n",
        "train_set, test_set=DATA_pre_pros(filename_pos,filename_neg)\n",
        "\n",
        "# LEARNNING_RATE=0.00015\n",
        "\n",
        "def main():\n",
        "    model = NetWork(INPUT_DIM, 2)\n",
        "    print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "    optimizer = optim.Adam(model.parameters(),lr=LEARNNING_RATE)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    criterion = criterion.to(device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    EPOCHS = 50\n",
        "\n",
        "    best_valid_loss = float('inf')\n",
        "    train_per_ep=[]\n",
        "    test_per_ep=[]\n",
        "    test_acc = 0\n",
        "    for epoch in range(EPOCHS):\n",
        "\n",
        "\n",
        "        train_x,train_y=shuffle_data(train_set)\n",
        "        train_iterator_x = torch.from_numpy(train_x.astype('float32')).split(64)\n",
        "        train_iterator_y = torch.from_numpy(train_y.astype('int64')).split(64)\n",
        "\n",
        "        test_x,test_y=shuffle_data(test_set)\n",
        "        test_iterator_x = torch.from_numpy(test_x.astype('float32')).split(64)\n",
        "        test_iterator_y = torch.from_numpy(test_y.astype('int64')).split(64)\n",
        "\n",
        "        train_loss, train_acc = train(model, train_iterator_x, train_iterator_y, optimizer, criterion, device)\n",
        "\n",
        "        test_loss, test_acc = evaluate(model, test_iterator_x, test_iterator_y,criterion, device)\n",
        "        train_per_ep.append(train_loss)\n",
        "        test_per_ep.append(test_loss)\n",
        "        \n",
        "        print(f'Epoch: {epoch + 1:02}')\n",
        "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')\n",
        "        print(f'Epoch: {epoch + 1:02}')\n",
        "        print(f'\\tTest Loss: {test_loss:.3f} | Test Acc: {test_acc * 100:.2f}%')\n",
        "\n",
        "    train_per_ep=np.array(train_per_ep)\n",
        "    test_per_ep=np.array(test_per_ep)\n",
        "    epocs=np.arange(1,EPOCHS+1)\n",
        "    # plt.figure()\n",
        "    # plt.plot(epocs,train_per_ep)\n",
        "    # plt.plot(epocs,test_per_ep)\n",
        "    # plt.legend(['train loss','test loss'])\n",
        "    # plt.show()\n",
        "    return test_acc, np.mean(np.abs(np.array(train_per_ep) - np.array(test_per_ep)))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkTUKuGfTuIE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 783
        },
        "outputId": "6a7725d9-ee5c-4995-c4b8-ce488b92d11e"
      },
      "source": [
        "learning_rate = np.linspace(0.0001, 0.001, num=10)\n",
        "input_1 = [70, 100, 130, 150]\n",
        "input_2 = [30, 40, 50, 60]\n",
        "input_3 = [10, 15, 20, 25]\n",
        "p = [0.2, 0.25, 0.35, 0.42, 0.5] \n",
        "acc = list()\n",
        "overfit=list()\n",
        "all=np.load('/content/drive/MyDrive/Colab Notebooks/opt.npy')\n",
        "for rate in learning_rate:\n",
        "  for input_1_ in input_1:\n",
        "    for input_2_ in input_2:\n",
        "      for input_3_ in input_3:\n",
        "        for p_ in p:\n",
        "            LEARNNING_RATE = rate\n",
        "            INPUT_1 = input_1_\n",
        "            INPUT_2 = input_2_\n",
        "            INPUT_3 = input_3_\n",
        "            P_DROPOUT = p_ \n",
        "          if np.array([INPUT_1,INPUT_2,INPUT_3,P_DROPOUT,LEARNNING_RATE]) in all[:,0:-2]:\n",
        "            continue\n",
        "          else:  \n",
        "            ac,of = main()\n",
        "            acc.append(ac)\n",
        "            overfit.append(of)\n",
        "            params=np.array([INPUT_1,INPUT_2,INPUT_3,P_DROPOUT,LEARNNING_RATE, ac, of])\n",
        "\n",
        "          \n",
        "          all=np.vstack([all, params])\n",
        "          np.save('/content/drive/MyDrive/Colab Notebooks/opt.npy,all)\n",
        "        \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 15,132 trainable parameters\n",
            "Epoch: 01\n",
            "\tTrain Loss: 0.529 | Train Acc: 80.54%\n",
            "Epoch: 01\n",
            "\tTest Loss: 0.301 | Test Acc: 89.55%\n",
            "Epoch: 02\n",
            "\tTrain Loss: 0.273 | Train Acc: 89.34%\n",
            "Epoch: 02\n",
            "\tTest Loss: 0.246 | Test Acc: 90.35%\n",
            "The model has 15,132 trainable parameters\n",
            "Epoch: 01\n",
            "\tTrain Loss: 0.623 | Train Acc: 60.51%\n",
            "Epoch: 01\n",
            "\tTest Loss: 0.349 | Test Acc: 88.53%\n",
            "Epoch: 02\n",
            "\tTrain Loss: 0.286 | Train Acc: 88.92%\n",
            "Epoch: 02\n",
            "\tTest Loss: 0.251 | Test Acc: 90.46%\n",
            "The model has 15,132 trainable parameters\n",
            "Epoch: 01\n",
            "\tTrain Loss: 0.545 | Train Acc: 72.76%\n",
            "Epoch: 01\n",
            "\tTest Loss: 0.291 | Test Acc: 88.88%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-f949c7c67c6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m           \u001b[0mINPUT_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_3_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m           \u001b[0mP_DROPOUT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m           \u001b[0mac\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mof\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m           \u001b[0macc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mac\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m           \u001b[0moverfit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mof\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-4c2798320f1f>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mtest_iterator_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'int64'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_iterator_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_iterator_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-8a91d5f91614>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator_x, iterator_y, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZWo5iKwKtfD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "179a0c34-5644-40b4-f268-6737d29d3d00"
      },
      "source": [
        "a=np.load('/content/drive/MyDrive/Colab Notebooks/opt.npy')\n",
        "print(np.array([7.0e+01,3.0e+01,1.0e+01,2.0e-01,1.0e-04]) in a[:,0:-2])\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    }
  ]
}