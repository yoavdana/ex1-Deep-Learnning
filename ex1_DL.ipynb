{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ex1_DL.ipynb",
      "provenance": [],
      "mount_file_id": "https://github.com/yoavdana/ex1_DL/blob/main/ex1_DL.ipynb",
      "authorship_tag": "ABX9TyPMdCw356qFeCDskHBSnj0B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoavdana/ex1_DL/blob/main/ex1_DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tNlOLLOv5q8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8ae7b65-eedb-4ec1-ad52-bfafe9379b39"
      },
      "source": [
        "!git clone https://github.com/yoavdana/ex1_DL.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ex1_DL'...\n",
            "remote: Enumerating objects: 77, done.\u001b[K\n",
            "remote: Counting objects: 100% (77/77), done.\u001b[K\n",
            "remote: Compressing objects: 100% (66/66), done.\u001b[K\n",
            "remote: Total 77 (delta 30), reused 30 (delta 7), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (77/77), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9c1XQ2OEx8t"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import scipy.stats as si\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "INPUT_DIM = 9 * 20\n",
        "OUTPUT_DIM = 2\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5V0o627FZyo"
      },
      "source": [
        "SEQ_LENGTH=20\n",
        "BOOTSTRAP_SIZE=24000\n",
        "NUMBER_OF_BATCHS=5\n",
        "\n",
        "\n",
        "\n",
        "def data_to_input(sequence, pos_or_neg):\n",
        "    mapping = {'A': 0, 'R': 1, 'N': 2, 'D': 3, 'C': 4, 'E': 5, 'Q': 6, 'G': 7, 'H': 8, 'I': 9, 'L': 10, 'K': 11,\n",
        "               'M': 12, 'F': 13, 'P': 14, 'S': 15, 'T': 16, 'W': 17, 'Y': 18, 'V': 19}\n",
        "    map=np.zeros((9, 20))\n",
        "    for i, seq in enumerate(sequence):\n",
        "        map[i,mapping[seq]]+=1\n",
        "    map = map.flatten()\n",
        "    return np.concatenate([map, np.array([pos_or_neg])])\n",
        "\n",
        "\n",
        "def Read_Data(filename, pos_or_neg):\n",
        "    file = open(filename, 'r')\n",
        "    lines=file.readlines()\n",
        "    if pos_or_neg==1:\n",
        "        DATA=np.zeros((len(lines), 181))\n",
        "        for i, line in enumerate(lines):\n",
        "            input = data_to_input(line.replace('\\n', ''), pos_or_neg)\n",
        "            DATA[i] = input\n",
        "        DATA=bootstrap(DATA, BOOTSTRAP_SIZE, NUMBER_OF_BATCHS)\n",
        "    else:\n",
        "        DATA = np.zeros((len(lines), 181))\n",
        "        for i, line in enumerate(lines):\n",
        "            input = data_to_input(line.replace('\\n', ''), pos_or_neg)\n",
        "            DATA[i] = input\n",
        "    return DATA\n",
        "\n",
        "\n",
        "def bootstrap(DATA,size,NUMBER_OF_BATCHS):\n",
        "\n",
        "    new_DATA=np.zeros((size,181))\n",
        "    N=DATA.shape[0]\n",
        "    batch_size=N//NUMBER_OF_BATCHS\n",
        "    for i in range(NUMBER_OF_BATCHS):\n",
        "\n",
        "        random = np.random.randint(batch_size*i,batch_size*(i+1), size=size//NUMBER_OF_BATCHS)\n",
        "        new_DATA[((size//NUMBER_OF_BATCHS)*i):(size//NUMBER_OF_BATCHS)*(i+1), :] = DATA[random, :]\n",
        "    return new_DATA\n",
        "\n",
        "\n",
        "def DATA_pre_pros(filename_pos,filename_neg):\n",
        "\n",
        "    neg_data=Read_Data(filename_neg, 0)\n",
        "    pos_data=Read_Data(filename_pos, 1)\n",
        "    final_data = np.concatenate([neg_data, pos_data])\n",
        "    np.random.shuffle(final_data)\n",
        "    train_set = final_data[:int(len(final_data)*0.9)]\n",
        "    test_set = final_data[int(len(final_data)*0.9):]\n",
        "    return train_set, test_set\n",
        "\n",
        "\n",
        "\n",
        "def shuffle_data(data_Xy):\n",
        "    np.random.shuffle(data_Xy)\n",
        "    return data_Xy[:,:180],data_Xy[:,-1]\n",
        "\n",
        "\n",
        "def spike_seq(filename):\n",
        "    \n",
        "    mapping = {'A': 0, 'R': 1, 'N': 2, 'D': 3, 'C': 4, 'E': 5, 'Q': 6, 'G': 7, 'H': 8, 'I': 9, 'L': 10, 'K': 11,\n",
        "               'M': 12, 'F': 13, 'P': 14, 'S': 15, 'T': 16, 'W': 17, 'Y': 18, 'V': 19}\n",
        "    \n",
        "    \n",
        "    with open(filename) as f:\n",
        "        lines = f.readlines()[0]\n",
        "        predeict=list()\n",
        "             \n",
        "        for i in range(len(lines)-9):\n",
        "            map = np.zeros((9, 20))\n",
        "            map[i, mapping[lines[i:i+9]]] += 1\n",
        "            map = map.flatten()\n",
        "            predeict.append(map)\n",
        "        \n",
        "        return predeict\n",
        "\n",
        "\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HutCXePsE9KO"
      },
      "source": [
        "INPUT_1=70\n",
        "INPUT_2=60\n",
        "INPUT_3=20\n",
        "P_DROPOUT=0.5\n",
        "\n",
        "\n",
        "class NetWork(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_fc = nn.Linear(input_dim, INPUT_1)\n",
        "        \n",
        "        self.hidden_1_fc = nn.Linear(INPUT_1, INPUT_2)\n",
        "        \n",
        "        self.hidden_2_fc = nn.Linear(INPUT_2, INPUT_3)\n",
        "        \n",
        "        self.output_fc = nn.Linear(INPUT_3, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(P_DROPOUT)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "   \n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "\n",
        "        h_1 = F.relu(self.input_fc(x))\n",
        "\n",
        "        h_1=self.dropout(h_1)\n",
        "       \n",
        "\n",
        "        h_2 = F.relu(self.hidden_1_fc(h_1))\n",
        "\n",
        "\n",
        "        h_3 = F.relu(self.hidden_2_fc(h_2))\n",
        "\n",
        "\n",
        "        y_pred = self.output_fc(h_3)\n",
        "\n",
        "       \n",
        "        \n",
        "        return y_pred, h_2\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1jysnYBFurA"
      },
      "source": [
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def calculate_accuracy(y_pred, y):\n",
        "    top_pred = y_pred.argmax(1, keepdim = True)\n",
        "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
        "    acc = correct.float() / y.shape[0]\n",
        "    return acc\n",
        "\n",
        "\n",
        "def train(model, iterator_x, iterator_y, optimizer, criterion, device):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for i, x in enumerate(iterator_x):\n",
        "        x = x.to(device)\n",
        "        y = iterator_y[i].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        y_pred, _ = model(x)\n",
        "\n",
        "        loss = criterion(y_pred, y)\n",
        "\n",
        "        acc = calculate_accuracy(y_pred, y)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator_x), epoch_acc / len(iterator_x)\n",
        "\n",
        "\n",
        "def evaluate(model, iterator_x, iterator_y, criterion, device):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for i, x in enumerate(iterator_x):\n",
        "        x = x.to(device)\n",
        "        y = iterator_y[i].to(device)\n",
        "\n",
        "        y_pred, _ = model(x)\n",
        "\n",
        "        loss = criterion(y_pred, y)\n",
        "\n",
        "        acc = calculate_accuracy(y_pred, y)\n",
        "\n",
        " \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator_x), epoch_acc / len(iterator_x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNHufHg2Gyz5"
      },
      "source": [
        "filename_pos='/content/ex1_DL/resorces/pos_A0201.txt'\n",
        "filename_neg='/content/ex1_DL/resorces/neg_A0201.txt'\n",
        "\n",
        "train_set, test_set=DATA_pre_pros(filename_pos,filename_neg)\n",
        "\n",
        "# LEARNNING_RATE=0.00015\n",
        "\n",
        "def main():\n",
        "    model = NetWork(INPUT_DIM, 2)\n",
        "    print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "    optimizer = optim.Adam(model.parameters(),lr=LEARNNING_RATE)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    criterion = criterion.to(device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    EPOCHS = 80\n",
        "\n",
        "    best_valid_loss = float('inf')\n",
        "    train_per_ep=[]\n",
        "    test_per_ep=[]\n",
        "    test_acc = 0\n",
        "    for epoch in range(EPOCHS):\n",
        "\n",
        "\n",
        "        train_x,train_y=shuffle_data(train_set)\n",
        "        train_iterator_x = torch.from_numpy(train_x.astype('float32')).split(64)\n",
        "        train_iterator_y = torch.from_numpy(train_y.astype('int64')).split(64)\n",
        "\n",
        "        test_x,test_y=shuffle_data(test_set)\n",
        "        test_iterator_x = torch.from_numpy(test_x.astype('float32')).split(64)\n",
        "        test_iterator_y = torch.from_numpy(test_y.astype('int64')).split(64)\n",
        "\n",
        "        train_loss, train_acc = train(model, train_iterator_x, train_iterator_y, optimizer, criterion, device)\n",
        "\n",
        "        test_loss, test_acc = evaluate(model, test_iterator_x, test_iterator_y,criterion, device)\n",
        "        train_per_ep.append(train_loss)\n",
        "        test_per_ep.append(test_loss)\n",
        "        \n",
        "        print(f'Epoch: {epoch + 1:02}')\n",
        "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')\n",
        "        print(f'Epoch: {epoch + 1:02}')\n",
        "        print(f'\\tTest Loss: {test_loss:.3f} | Test Acc: {test_acc * 100:.2f}%')\n",
        "\n",
        "    train_per_ep=np.array(train_per_ep)\n",
        "    test_per_ep=np.array(test_per_ep)\n",
        "    epocs=np.arange(1,EPOCHS+1)\n",
        "    plt.figure()\n",
        "    plt.plot(epocs,train_per_ep)\n",
        "    plt.plot(epocs,test_per_ep)\n",
        "    plt.legend(['train loss','test loss'])\n",
        "    plt.show()\n",
        "    return test_acc, np.mean(np.abs(np.array(train_per_ep) - np.array(test_per_ep)))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heAumw8VTJUC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkTUKuGfTuIE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6b096140-d9c7-4342-8fa6-e7bd25bdd58e"
      },
      "source": [
        "# learning_rate = np.linspace(0.0001, 0.001, num=10)\n",
        "# input_1 = [70, 100, 130, 150]\n",
        "# input_2 = [30, 40, 50, 60]\n",
        "# input_3 = [10, 15, 20, 25]\n",
        "# p = [0.2, 0.25, 0.35, 0.42, 0.5] \n",
        "acc = list()\n",
        "overfit=list()\n",
        "\n",
        "input_1=[70]\n",
        "input_2=[60]\n",
        "input_3=[20]\n",
        "p=[0.5]\n",
        "learning_rate=[0.0002]\n",
        "\n",
        "\n",
        "all=np.load('/content/drive/MyDrive/Colab Notebooks/opt.npy',allow_pickle=True)\n",
        "i=0\n",
        "for rate in learning_rate:\n",
        "  for input_1_ in input_1:\n",
        "    for input_2_ in input_2:\n",
        "      for input_3_ in input_3:\n",
        "        for p_ in p:\n",
        "            LEARNNING_RATE = rate\n",
        "            INPUT_1 = input_1_\n",
        "            INPUT_2 = input_2_\n",
        "            INPUT_3 = input_3_\n",
        "            P_DROPOUT = p_ \n",
        "\n",
        "            if any((np.array([INPUT_1,INPUT_2,INPUT_3,P_DROPOUT,LEARNNING_RATE])==all[:,0:-2]).all(1)):\n",
        "              print(i)\n",
        "              i+=1\n",
        "              continue\n",
        "            else:  \n",
        "              ac,of = main()\n",
        "              acc.append(ac)\n",
        "              overfit.append(of)\n",
        "              params=np.array([INPUT_1,INPUT_2,INPUT_3,P_DROPOUT,LEARNNING_RATE, ac, of])\n",
        "              all=np.vstack([all, params])\n",
        "              #np.save('/content/drive/MyDrive/Colab Notebooks/opt.npy',all)\n",
        "          \n",
        "\n",
        "        \n",
        "\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 18,192 trainable parameters\n",
            "tensor([0.1650, 0.8350], grad_fn=<SoftmaxBackward>) tensor(1)\n",
            "tensor([0.0758, 0.9242], grad_fn=<SoftmaxBackward>) tensor(1)\n",
            "tensor([0.3070, 0.6930], grad_fn=<SoftmaxBackward>) tensor(0)\n",
            "tensor([0.3203, 0.6797], grad_fn=<SoftmaxBackward>) tensor(0)\n",
            "tensor([0.1124, 0.8876], grad_fn=<SoftmaxBackward>) tensor(1)\n",
            "tensor([0.9886, 0.0114], grad_fn=<SoftmaxBackward>) tensor(0)\n",
            "tensor([0.5179, 0.4821], grad_fn=<SoftmaxBackward>) tensor(0)\n",
            "tensor([0.9962, 0.0038], grad_fn=<SoftmaxBackward>) tensor(0)\n",
            "tensor([0.0704, 0.9296], grad_fn=<SoftmaxBackward>) tensor(1)\n",
            "tensor([0.9906, 0.0094], grad_fn=<SoftmaxBackward>) tensor(0)\n",
            "tensor([0.0736, 0.9264], grad_fn=<SoftmaxBackward>) tensor(1)\n",
            "tensor([0.9623, 0.0377], grad_fn=<SoftmaxBackward>) tensor(0)\n",
            "tensor([0.1662, 0.8338], grad_fn=<SoftmaxBackward>) tensor(1)\n",
            "tensor([0.9990, 0.0011], grad_fn=<SoftmaxBackward>) tensor(0)\n",
            "tensor([0.0959, 0.9041], grad_fn=<SoftmaxBackward>) tensor(1)\n",
            "tensor([0.1051, 0.8949], grad_fn=<SoftmaxBackward>) tensor(1)\n",
            "tensor([0.9973, 0.0027], grad_fn=<SoftmaxBackward>) tensor(0)\n",
            "tensor([0.9980, 0.0020], grad_fn=<SoftmaxBackward>) tensor(0)\n",
            "tensor([0.9809, 0.0191], grad_fn=<SoftmaxBackward>) tensor(0)\n",
            "tensor([0.0615, 0.9385], grad_fn=<SoftmaxBackward>) tensor(1)\n",
            "tensor([0.9907, 0.0093], grad_fn=<SoftmaxBackward>) tensor(0)\n",
            "tensor([0.9901, 0.0099], grad_fn=<SoftmaxBackward>) tensor(0)\n",
            "tensor([0.1170, 0.8830], grad_fn=<SoftmaxBackward>) tensor(1)\n",
            "tensor([0.9988, 0.0012], grad_fn=<SoftmaxBackward>) tensor(0)\n",
            "tensor([0.0622, 0.9378], grad_fn=<SoftmaxBackward>) tensor(1)\n",
            "tensor([0.0804, 0.9196], grad_fn=<SoftmaxBackward>) tensor(1)\n",
            "tensor([0.0883, 0.9117], grad_fn=<SoftmaxBackward>) tensor(1)\n",
            "tensor([0.0842, 0.9158], grad_fn=<SoftmaxBackward>) tensor(1)\n",
            "tensor([0.9124, 0.0876], grad_fn=<SoftmaxBackward>) tensor(0)\n",
            "tensor([9.9943e-01, 5.7212e-04], grad_fn=<SoftmaxBackward>) tensor(0)\n",
            "tensor([0.9887, 0.0113], grad_fn=<SoftmaxBackward>) tensor(0)\n",
            "tensor([0.1097, 0.8903], grad_fn=<SoftmaxBackward>) tensor(1)\n",
            "tensor([0.0615, 0.9385], grad_fn=<SoftmaxBackward>) tensor(1)\n",
            "tensor([0.0615, 0.9385], grad_fn=<SoftmaxBackward>) tensor(1)\n",
            "tensor([0.1178, 0.8822], grad_fn=<SoftmaxBackward>) tensor(1)\n",
            "tensor([0.1102, 0.8898], grad_fn=<SoftmaxBackward>) tensor(1)\n",
            "tensor([0.7247, 0.2753], grad_fn=<SoftmaxBackward>) tensor(0)\n",
            "tensor([0.0674, 0.9326], grad_fn=<SoftmaxBackward>) tensor(1)\n",
            "tensor([0.7655, 0.2345], grad_fn=<SoftmaxBackward>) tensor(0)\n",
            "tensor([0.9610, 0.0390], grad_fn=<SoftmaxBackward>) tensor(0)\n",
            "tensor([0.9880, 0.0120], grad_fn=<SoftmaxBackward>) tensor(0)\n",
            "tensor([0.9981, 0.0019], grad_fn=<SoftmaxBackward>) tensor(0)\n",
            "tensor([0.9872, 0.0128], grad_fn=<SoftmaxBackward>) tensor(0)\n",
            "tensor([0.0725, 0.9275], grad_fn=<SoftmaxBackward>) tensor(1)\n",
            "tensor([0.3691, 0.6309], grad_fn=<SoftmaxBackward>) tensor(1)\n",
            "tensor([0.2504, 0.7496], grad_fn=<SoftmaxBackward>) tensor(1)\n",
            "tensor([0.9953, 0.0047], grad_fn=<SoftmaxBackward>) tensor(0)\n",
            "tensor([0.0819, 0.9181], grad_fn=<SoftmaxBackward>) tensor(1)\n",
            "tensor([0.2964, 0.7036], grad_fn=<SoftmaxBackward>) tensor(0)\n",
            "tensor([0.9931, 0.0069], grad_fn=<SoftmaxBackward>) tensor(0)\n",
            "tensor([0.1132, 0.8868], grad_fn=<SoftmaxBackward>) tensor(1)\n",
            "tensor([0.9959, 0.0041], grad_fn=<SoftmaxBackward>) tensor(0)\n",
            "tensor([0.0544, 0.9456], grad_fn=<SoftmaxBackward>) tensor(1)\n",
            "tensor([0.0967, 0.9033], grad_fn=<SoftmaxBackward>) tensor(0)\n",
            "tensor([0.2125, 0.7875], grad_fn=<SoftmaxBackward>) tensor(1)\n",
            "tensor([0.0763, 0.9237], grad_fn=<SoftmaxBackward>) tensor(1)\n",
            "tensor([0.9983, 0.0017], grad_fn=<SoftmaxBackward>) tensor(0)\n",
            "tensor([0.9928, 0.0072], grad_fn=<SoftmaxBackward>) tensor(0)\n",
            "tensor([0.9894, 0.0106], grad_fn=<SoftmaxBackward>) tensor(0)\n",
            "tensor([0.0990, 0.9010], grad_fn=<SoftmaxBackward>) tensor(1)\n",
            "tensor([0.9912, 0.0088], grad_fn=<SoftmaxBackward>) tensor(0)\n",
            "tensor([0.2178, 0.7822], grad_fn=<SoftmaxBackward>) tensor(1)\n",
            "tensor([0.1633, 0.8367], grad_fn=<SoftmaxBackward>) tensor(1)\n",
            "tensor([0.9435, 0.0565], grad_fn=<SoftmaxBackward>) tensor(0)\n",
            "tensor([0.9975, 0.0025], grad_fn=<SoftmaxBackward>) tensor(0)\n",
            "tensor([0.0377, 0.9623], grad_fn=<SoftmaxBackward>) tensor(1)\n",
            "tensor([0.0378, 0.9622], grad_fn=<SoftmaxBackward>) tensor(1)\n",
            "tensor([0.9957, 0.0043], grad_fn=<SoftmaxBackward>) tensor(0)\n",
            "tensor([0.9979, 0.0021], grad_fn=<SoftmaxBackward>) tensor(0)\n",
            "tensor([0.1216, 0.8784], grad_fn=<SoftmaxBackward>) tensor(1)\n",
            "tensor([0.3376, 0.6624], grad_fn=<SoftmaxBackward>) tensor(1)\n",
            "tensor([0.9853, 0.0147], grad_fn=<SoftmaxBackward>) tensor(0)\n",
            "tensor([0.0494, 0.9506], grad_fn=<SoftmaxBackward>) tensor(1)\n",
            "tensor([0.1063, 0.8937], grad_fn=<SoftmaxBackward>) tensor(1)\n",
            "tensor([0.2098, 0.7902], grad_fn=<SoftmaxBackward>) tensor(1)\n",
            "tensor([0.0923, 0.9077], grad_fn=<SoftmaxBackward>) tensor(1)\n",
            "Epoch: 01\n",
            "\tTrain Loss: 0.426 | Train Acc: 78.46%\n",
            "Epoch: 01\n",
            "\tTest Loss: 0.249 | Test Acc: 90.19%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZWo5iKwKtfD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "eb082df4-bba8-4383-d1d0-33df7e98d662"
      },
      "source": [
        "filename_pos='/content/ex1_DL/resorces/pos_A0201.txt'\n",
        "filename_neg='/content/ex1_DL/resorces/neg_A0201.txt'\n",
        "\n",
        "train_set, test_set=DATA_pre_pros(filename_pos,filename_neg)\n",
        "\n",
        "LEARNNING_RATE=0.0002\n",
        "\n",
        "\n",
        "model = NetWork(INPUT_DIM, 2)\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "optimizer = optim.Adam(model.parameters(),lr=LEARNNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "EPOCHS = 60\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "train_per_ep=[]\n",
        "test_per_ep=[]\n",
        "test_acc = 0\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "\n",
        "    train_x,train_y=shuffle_data(train_set)\n",
        "    train_iterator_x = torch.from_numpy(train_x.astype('float32')).split(64)\n",
        "    train_iterator_y = torch.from_numpy(train_y.astype('int64')).split(64)\n",
        "\n",
        "    test_x,test_y=shuffle_data(test_set)\n",
        "    test_iterator_x = torch.from_numpy(test_x.astype('float32')).split(64)\n",
        "    test_iterator_y = torch.from_numpy(test_y.astype('int64')).split(64)\n",
        "\n",
        "    train_loss, train_acc = train(model, train_iterator_x, train_iterator_y, optimizer, criterion, device)\n",
        "\n",
        "    test_loss, test_acc = evaluate(model, test_iterator_x, test_iterator_y,criterion, device)\n",
        "    train_per_ep.append(train_loss)\n",
        "    test_per_ep.append(test_loss)\n",
        "    \n",
        "    print(f'Epoch: {epoch + 1:02}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')\n",
        "    print(f'Epoch: {epoch + 1:02}')\n",
        "    print(f'\\tTest Loss: {test_loss:.3f} | Test Acc: {test_acc * 100:.2f}%')\n",
        "\n",
        "train_per_ep=np.array(train_per_ep)\n",
        "test_per_ep=np.array(test_per_ep)\n",
        "epocs=np.arange(1,EPOCHS+1)\n",
        "plt.figure()\n",
        "plt.plot(epocs,train_per_ep)\n",
        "plt.plot(epocs,test_per_ep)\n",
        "plt.legend(['train loss','test loss'])\n",
        "plt.show()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 18,192 trainable parameters\n",
            "Epoch: 01\n",
            "\tTrain Loss: 0.412 | Train Acc: 82.43%\n",
            "Epoch: 01\n",
            "\tTest Loss: 0.258 | Test Acc: 89.85%\n",
            "Epoch: 02\n",
            "\tTrain Loss: 0.262 | Train Acc: 89.76%\n",
            "Epoch: 02\n",
            "\tTest Loss: 0.237 | Test Acc: 91.02%\n",
            "Epoch: 03\n",
            "\tTrain Loss: 0.245 | Train Acc: 90.40%\n",
            "Epoch: 03\n",
            "\tTest Loss: 0.227 | Test Acc: 91.42%\n",
            "Epoch: 04\n",
            "\tTrain Loss: 0.233 | Train Acc: 91.02%\n",
            "Epoch: 04\n",
            "\tTest Loss: 0.215 | Test Acc: 91.73%\n",
            "Epoch: 05\n",
            "\tTrain Loss: 0.223 | Train Acc: 91.36%\n",
            "Epoch: 05\n",
            "\tTest Loss: 0.207 | Test Acc: 92.21%\n",
            "Epoch: 06\n",
            "\tTrain Loss: 0.214 | Train Acc: 91.96%\n",
            "Epoch: 06\n",
            "\tTest Loss: 0.199 | Test Acc: 92.45%\n",
            "Epoch: 07\n",
            "\tTrain Loss: 0.204 | Train Acc: 92.48%\n",
            "Epoch: 07\n",
            "\tTest Loss: 0.190 | Test Acc: 92.84%\n",
            "Epoch: 08\n",
            "\tTrain Loss: 0.198 | Train Acc: 92.72%\n",
            "Epoch: 08\n",
            "\tTest Loss: 0.184 | Test Acc: 93.21%\n",
            "Epoch: 09\n",
            "\tTrain Loss: 0.189 | Train Acc: 93.12%\n",
            "Epoch: 09\n",
            "\tTest Loss: 0.175 | Test Acc: 93.71%\n",
            "Epoch: 10\n",
            "\tTrain Loss: 0.181 | Train Acc: 93.52%\n",
            "Epoch: 10\n",
            "\tTest Loss: 0.168 | Test Acc: 94.13%\n",
            "Epoch: 11\n",
            "\tTrain Loss: 0.175 | Train Acc: 93.85%\n",
            "Epoch: 11\n",
            "\tTest Loss: 0.162 | Test Acc: 94.59%\n",
            "Epoch: 12\n",
            "\tTrain Loss: 0.167 | Train Acc: 94.05%\n",
            "Epoch: 12\n",
            "\tTest Loss: 0.155 | Test Acc: 94.90%\n",
            "Epoch: 13\n",
            "\tTrain Loss: 0.161 | Train Acc: 94.41%\n",
            "Epoch: 13\n",
            "\tTest Loss: 0.150 | Test Acc: 94.99%\n",
            "Epoch: 14\n",
            "\tTrain Loss: 0.156 | Train Acc: 94.65%\n",
            "Epoch: 14\n",
            "\tTest Loss: 0.149 | Test Acc: 95.07%\n",
            "Epoch: 15\n",
            "\tTrain Loss: 0.150 | Train Acc: 94.92%\n",
            "Epoch: 15\n",
            "\tTest Loss: 0.144 | Test Acc: 95.17%\n",
            "Epoch: 16\n",
            "\tTrain Loss: 0.147 | Train Acc: 95.02%\n",
            "Epoch: 16\n",
            "\tTest Loss: 0.140 | Test Acc: 95.29%\n",
            "Epoch: 17\n",
            "\tTrain Loss: 0.140 | Train Acc: 95.24%\n",
            "Epoch: 17\n",
            "\tTest Loss: 0.130 | Test Acc: 95.75%\n",
            "Epoch: 18\n",
            "\tTrain Loss: 0.136 | Train Acc: 95.50%\n",
            "Epoch: 18\n",
            "\tTest Loss: 0.127 | Test Acc: 95.99%\n",
            "Epoch: 19\n",
            "\tTrain Loss: 0.131 | Train Acc: 95.72%\n",
            "Epoch: 19\n",
            "\tTest Loss: 0.124 | Test Acc: 96.12%\n",
            "Epoch: 20\n",
            "\tTrain Loss: 0.130 | Train Acc: 95.69%\n",
            "Epoch: 20\n",
            "\tTest Loss: 0.121 | Test Acc: 96.21%\n",
            "Epoch: 21\n",
            "\tTrain Loss: 0.124 | Train Acc: 95.90%\n",
            "Epoch: 21\n",
            "\tTest Loss: 0.121 | Test Acc: 96.18%\n",
            "Epoch: 22\n",
            "\tTrain Loss: 0.119 | Train Acc: 96.16%\n",
            "Epoch: 22\n",
            "\tTest Loss: 0.114 | Test Acc: 96.48%\n",
            "Epoch: 23\n",
            "\tTrain Loss: 0.119 | Train Acc: 96.23%\n",
            "Epoch: 23\n",
            "\tTest Loss: 0.116 | Test Acc: 96.25%\n",
            "Epoch: 24\n",
            "\tTrain Loss: 0.112 | Train Acc: 96.45%\n",
            "Epoch: 24\n",
            "\tTest Loss: 0.112 | Test Acc: 96.39%\n",
            "Epoch: 25\n",
            "\tTrain Loss: 0.111 | Train Acc: 96.51%\n",
            "Epoch: 25\n",
            "\tTest Loss: 0.108 | Test Acc: 96.67%\n",
            "Epoch: 26\n",
            "\tTrain Loss: 0.108 | Train Acc: 96.60%\n",
            "Epoch: 26\n",
            "\tTest Loss: 0.106 | Test Acc: 96.73%\n",
            "Epoch: 27\n",
            "\tTrain Loss: 0.107 | Train Acc: 96.68%\n",
            "Epoch: 27\n",
            "\tTest Loss: 0.103 | Test Acc: 96.89%\n",
            "Epoch: 28\n",
            "\tTrain Loss: 0.099 | Train Acc: 96.97%\n",
            "Epoch: 28\n",
            "\tTest Loss: 0.102 | Test Acc: 96.91%\n",
            "Epoch: 29\n",
            "\tTrain Loss: 0.100 | Train Acc: 96.97%\n",
            "Epoch: 29\n",
            "\tTest Loss: 0.101 | Test Acc: 96.95%\n",
            "Epoch: 30\n",
            "\tTrain Loss: 0.097 | Train Acc: 97.04%\n",
            "Epoch: 30\n",
            "\tTest Loss: 0.100 | Test Acc: 97.03%\n",
            "Epoch: 31\n",
            "\tTrain Loss: 0.095 | Train Acc: 97.22%\n",
            "Epoch: 31\n",
            "\tTest Loss: 0.100 | Test Acc: 96.86%\n",
            "Epoch: 32\n",
            "\tTrain Loss: 0.094 | Train Acc: 97.17%\n",
            "Epoch: 32\n",
            "\tTest Loss: 0.091 | Test Acc: 97.25%\n",
            "Epoch: 33\n",
            "\tTrain Loss: 0.093 | Train Acc: 97.18%\n",
            "Epoch: 33\n",
            "\tTest Loss: 0.095 | Test Acc: 97.08%\n",
            "Epoch: 34\n",
            "\tTrain Loss: 0.090 | Train Acc: 97.34%\n",
            "Epoch: 34\n",
            "\tTest Loss: 0.097 | Test Acc: 97.10%\n",
            "Epoch: 35\n",
            "\tTrain Loss: 0.088 | Train Acc: 97.34%\n",
            "Epoch: 35\n",
            "\tTest Loss: 0.094 | Test Acc: 97.12%\n",
            "Epoch: 36\n",
            "\tTrain Loss: 0.086 | Train Acc: 97.51%\n",
            "Epoch: 36\n",
            "\tTest Loss: 0.091 | Test Acc: 97.44%\n",
            "Epoch: 37\n",
            "\tTrain Loss: 0.085 | Train Acc: 97.49%\n",
            "Epoch: 37\n",
            "\tTest Loss: 0.091 | Test Acc: 97.32%\n",
            "Epoch: 38\n",
            "\tTrain Loss: 0.080 | Train Acc: 97.68%\n",
            "Epoch: 38\n",
            "\tTest Loss: 0.087 | Test Acc: 97.45%\n",
            "Epoch: 39\n",
            "\tTrain Loss: 0.080 | Train Acc: 97.70%\n",
            "Epoch: 39\n",
            "\tTest Loss: 0.092 | Test Acc: 97.41%\n",
            "Epoch: 40\n",
            "\tTrain Loss: 0.082 | Train Acc: 97.65%\n",
            "Epoch: 40\n",
            "\tTest Loss: 0.086 | Test Acc: 97.59%\n",
            "Epoch: 41\n",
            "\tTrain Loss: 0.076 | Train Acc: 97.83%\n",
            "Epoch: 41\n",
            "\tTest Loss: 0.082 | Test Acc: 97.73%\n",
            "Epoch: 42\n",
            "\tTrain Loss: 0.077 | Train Acc: 97.77%\n",
            "Epoch: 42\n",
            "\tTest Loss: 0.085 | Test Acc: 97.67%\n",
            "Epoch: 43\n",
            "\tTrain Loss: 0.072 | Train Acc: 97.94%\n",
            "Epoch: 43\n",
            "\tTest Loss: 0.086 | Test Acc: 97.71%\n",
            "Epoch: 44\n",
            "\tTrain Loss: 0.073 | Train Acc: 97.89%\n",
            "Epoch: 44\n",
            "\tTest Loss: 0.084 | Test Acc: 97.69%\n",
            "Epoch: 45\n",
            "\tTrain Loss: 0.073 | Train Acc: 97.91%\n",
            "Epoch: 45\n",
            "\tTest Loss: 0.082 | Test Acc: 97.77%\n",
            "Epoch: 46\n",
            "\tTrain Loss: 0.071 | Train Acc: 98.07%\n",
            "Epoch: 46\n",
            "\tTest Loss: 0.079 | Test Acc: 97.81%\n",
            "Epoch: 47\n",
            "\tTrain Loss: 0.070 | Train Acc: 98.01%\n",
            "Epoch: 47\n",
            "\tTest Loss: 0.082 | Test Acc: 97.81%\n",
            "Epoch: 48\n",
            "\tTrain Loss: 0.071 | Train Acc: 97.97%\n",
            "Epoch: 48\n",
            "\tTest Loss: 0.080 | Test Acc: 97.88%\n",
            "Epoch: 49\n",
            "\tTrain Loss: 0.068 | Train Acc: 98.10%\n",
            "Epoch: 49\n",
            "\tTest Loss: 0.075 | Test Acc: 98.02%\n",
            "Epoch: 50\n",
            "\tTrain Loss: 0.068 | Train Acc: 98.10%\n",
            "Epoch: 50\n",
            "\tTest Loss: 0.078 | Test Acc: 97.98%\n",
            "Epoch: 51\n",
            "\tTrain Loss: 0.068 | Train Acc: 98.11%\n",
            "Epoch: 51\n",
            "\tTest Loss: 0.077 | Test Acc: 97.94%\n",
            "Epoch: 52\n",
            "\tTrain Loss: 0.066 | Train Acc: 98.11%\n",
            "Epoch: 52\n",
            "\tTest Loss: 0.081 | Test Acc: 98.01%\n",
            "Epoch: 53\n",
            "\tTrain Loss: 0.065 | Train Acc: 98.15%\n",
            "Epoch: 53\n",
            "\tTest Loss: 0.072 | Test Acc: 98.16%\n",
            "Epoch: 54\n",
            "\tTrain Loss: 0.066 | Train Acc: 98.19%\n",
            "Epoch: 54\n",
            "\tTest Loss: 0.078 | Test Acc: 97.96%\n",
            "Epoch: 55\n",
            "\tTrain Loss: 0.065 | Train Acc: 98.19%\n",
            "Epoch: 55\n",
            "\tTest Loss: 0.073 | Test Acc: 98.18%\n",
            "Epoch: 56\n",
            "\tTrain Loss: 0.063 | Train Acc: 98.16%\n",
            "Epoch: 56\n",
            "\tTest Loss: 0.079 | Test Acc: 98.01%\n",
            "Epoch: 57\n",
            "\tTrain Loss: 0.066 | Train Acc: 98.22%\n",
            "Epoch: 57\n",
            "\tTest Loss: 0.074 | Test Acc: 98.12%\n",
            "Epoch: 58\n",
            "\tTrain Loss: 0.062 | Train Acc: 98.22%\n",
            "Epoch: 58\n",
            "\tTest Loss: 0.074 | Test Acc: 98.11%\n",
            "Epoch: 59\n",
            "\tTrain Loss: 0.063 | Train Acc: 98.24%\n",
            "Epoch: 59\n",
            "\tTest Loss: 0.075 | Test Acc: 98.13%\n",
            "Epoch: 60\n",
            "\tTrain Loss: 0.062 | Train Acc: 98.26%\n",
            "Epoch: 60\n",
            "\tTest Loss: 0.078 | Test Acc: 98.07%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfftOmlOTnCL"
      },
      "source": [
        "\n",
        "seq=spike_seq('resorces/spike_acid.txt')\n",
        "\n",
        "z = y_pred\n",
        "        my_softmax = nn.Softmax(dim=1)\n",
        "        z = nn.Softmax(z)\n",
        "        print(z)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}