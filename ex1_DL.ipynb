{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ex1_DL.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOHx0EuWQmqBDt8UZV0tfZj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoavdana/ex1_DL/blob/main/ex1_DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tNlOLLOv5q8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48c11503-001f-4f77-871a-865f487000a5"
      },
      "source": [
        "!git clone https://github.com/yoavdana/ex1_DL.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ex1_DL'...\n",
            "remote: Enumerating objects: 32, done.\u001b[K\n",
            "remote: Counting objects:   3% (1/32)\u001b[K\rremote: Counting objects:   6% (2/32)\u001b[K\rremote: Counting objects:   9% (3/32)\u001b[K\rremote: Counting objects:  12% (4/32)\u001b[K\rremote: Counting objects:  15% (5/32)\u001b[K\rremote: Counting objects:  18% (6/32)\u001b[K\rremote: Counting objects:  21% (7/32)\u001b[K\rremote: Counting objects:  25% (8/32)\u001b[K\rremote: Counting objects:  28% (9/32)\u001b[K\rremote: Counting objects:  31% (10/32)\u001b[K\rremote: Counting objects:  34% (11/32)\u001b[K\rremote: Counting objects:  37% (12/32)\u001b[K\rremote: Counting objects:  40% (13/32)\u001b[K\rremote: Counting objects:  43% (14/32)\u001b[K\rremote: Counting objects:  46% (15/32)\u001b[K\rremote: Counting objects:  50% (16/32)\u001b[K\rremote: Counting objects:  53% (17/32)\u001b[K\rremote: Counting objects:  56% (18/32)\u001b[K\rremote: Counting objects:  59% (19/32)\u001b[K\rremote: Counting objects:  62% (20/32)\u001b[K\rremote: Counting objects:  65% (21/32)\u001b[K\rremote: Counting objects:  68% (22/32)\u001b[K\rremote: Counting objects:  71% (23/32)\u001b[K\rremote: Counting objects:  75% (24/32)\u001b[K\rremote: Counting objects:  78% (25/32)\u001b[K\rremote: Counting objects:  81% (26/32)\u001b[K\rremote: Counting objects:  84% (27/32)\u001b[K\rremote: Counting objects:  87% (28/32)\u001b[K\rremote: Counting objects:  90% (29/32)\u001b[K\rremote: Counting objects:  93% (30/32)\u001b[K\rremote: Counting objects:  96% (31/32)\u001b[K\rremote: Counting objects: 100% (32/32)\u001b[K\rremote: Counting objects: 100% (32/32), done.\u001b[K\n",
            "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
            "remote: Total 32 (delta 6), reused 27 (delta 5), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (32/32), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9c1XQ2OEx8t"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import scipy.stats as si\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import time\n",
        "\n",
        "INPUT_DIM = 9 * 20\n",
        "OUTPUT_DIM = 2\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5V0o627FZyo"
      },
      "source": [
        "SEQ_LENGTH=20\n",
        "BOOTSTRAP_SIZE=10000\n",
        "NUMBER_OF_BATCHS=4\n",
        "\n",
        "\n",
        "\n",
        "def data_to_input(sequence):\n",
        "    mapping = {'A': 0, 'R': 1, 'N': 2, 'D': 3, 'C': 4, 'E': 5, 'Q': 6, 'G': 7, 'H': 8, 'I': 9, 'L': 10, 'K': 11,\n",
        "               'M': 12, 'F': 13, 'P': 14, 'S': 15, 'T': 16, 'W': 17, 'Y': 18, 'V': 19}\n",
        "    map=np.zeros((9, 20))\n",
        "    for i, seq in enumerate(sequence):\n",
        "        map[i,mapping[seq]]+=1\n",
        "    return map\n",
        "\n",
        "\n",
        "def Read_Data(filename, pos_or_neg):\n",
        "    file = open(filename, 'r')\n",
        "    lines=file.readlines()\n",
        "    if pos_or_neg==1:\n",
        "        DATA=np.zeros((len(lines), 9, 20))\n",
        "        for i, line in enumerate(lines):\n",
        "            input = data_to_input(line.replace('\\n', ''))\n",
        "            DATA[i] = input\n",
        "        DATA=bootstrap(DATA, BOOTSTRAP_SIZE, NUMBER_OF_BATCHS)\n",
        "    else:\n",
        "        DATA = list()\n",
        "        for i, line in enumerate(lines):\n",
        "            input = data_to_input(line.replace('\\n', ''))\n",
        "            DATA.append((input, pos_or_neg))\n",
        "    return np.array(DATA)\n",
        "\n",
        "\n",
        "def bootstrap(DATA,size,NUMBER_OF_BATCHS):\n",
        "\n",
        "    new_DATA=np.zeros((size,9,20))\n",
        "    N=DATA.shape[0]\n",
        "    batch_size=N//NUMBER_OF_BATCHS\n",
        "    for i in range(NUMBER_OF_BATCHS):\n",
        "\n",
        "        random = np.random.randint(batch_size*i,batch_size*(i+1), size=size//NUMBER_OF_BATCHS)\n",
        "        new_DATA[((size//NUMBER_OF_BATCHS)*i):(size//NUMBER_OF_BATCHS)*(i+1), :, :] = DATA[random, :, :]\n",
        "    result = list()\n",
        "    for data in new_DATA:\n",
        "        result.append((data, 1))\n",
        "    return result\n",
        "\n",
        "def DATA_pre_pros(filename_pos,filename_neg):\n",
        "\n",
        "    neg_data=Read_Data(filename_neg, 0)\n",
        "    pos_data=Read_Data(filename_pos, 1)\n",
        "    final_data = np.concatenate([neg_data, pos_data])\n",
        "    np.random.shuffle(final_data)\n",
        "    train_set = final_data[:int(len(final_data)*0.9)]\n",
        "    test_set = final_data[int(len(final_data)*0.9):]\n",
        "    return train_set, test_set\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HutCXePsE9KO"
      },
      "source": [
        "class NetWork(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_fc = nn.Linear(input_dim, 60)\n",
        "        self.hidden_fc = nn.Linear(60, 25)\n",
        "        self.output_fc = nn.Linear(25, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = [batch size, height, width]\n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        x = x.view(batch_size, -1)\n",
        "\n",
        "        # x = [batch size, height * width]\n",
        "\n",
        "        h_1 = F.relu(self.input_fc(x))\n",
        "\n",
        "        # h_1 = [batch size, 250]\n",
        "\n",
        "        h_2 = F.relu(self.hidden_fc(h_1))\n",
        "\n",
        "        # h_2 = [batch size, 100]\n",
        "\n",
        "        y_pred = self.output_fc(h_2)\n",
        "\n",
        "        # y_pred = [batch size, output dim]\n",
        "\n",
        "        return y_pred, h_2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1jysnYBFurA"
      },
      "source": [
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def calculate_accuracy(y_pred, y):\n",
        "    top_pred = y_pred.argmax(1, keepdim = True)\n",
        "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
        "    acc = correct.float() / y.shape[0]\n",
        "    return acc\n",
        "\n",
        "\n",
        "def train(model, iterator, optimizer, criterion, device):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for (x, y) in iterator:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        y_pred, _ = model(x)\n",
        "\n",
        "        loss = criterion(y_pred, y)\n",
        "\n",
        "        acc = calculate_accuracy(y_pred, y)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "fNHufHg2Gyz5",
        "outputId": "c4b06803-7844-4431-fb77-1111b5da24fd"
      },
      "source": [
        "filename_pos='/content/ex1_DL/resorces/pos_A0201.txt'\n",
        "filename_neg='/content/ex1_DL/resorces/neg_A0201.txt'\n",
        "\n",
        "train_set_x, test_set_x=DATA_pre_pros(filename_pos,filename_neg)\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_iterator = data.DataLoader(train_set_x, \n",
        "                                 shuffle = True, \n",
        "                                 batch_size = BATCH_SIZE)\n",
        "\n",
        "test_iterator = data.DataLoader(test_set_x, \n",
        "                                batch_size = BATCH_SIZE)\n",
        "\n",
        "batch_size=64\n",
        "model = NetWork(INPUT_DIM, OUTPUT_DIM)\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "EPOCHS = 10\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  \n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, device)\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-c40262403f43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                                 batch_size = BATCH_SIZE)\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# batch_size=64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataLoader' object has no attribute 'value'"
          ]
        }
      ]
    }
  ]
}