{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ex1_DL.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoavdana/ex1_DL/blob/main/ex1_DL_copy1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tNlOLLOv5q8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65bd894d-0978-4a7b-94a0-ac8b2f2b8439"
      },
      "source": [
        "!git clone https://github.com/yoavdana/ex1_DL.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ex1_DL'...\n",
            "remote: Enumerating objects: 103, done.\u001b[K\n",
            "remote: Counting objects: 100% (103/103), done.\u001b[K\n",
            "remote: Compressing objects: 100% (87/87), done.\u001b[K\n",
            "remote: Total 103 (delta 36), reused 48 (delta 9), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (103/103), 59.22 MiB | 26.21 MiB/s, done.\n",
            "Resolving deltas: 100% (36/36), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9c1XQ2OEx8t"
      },
      "source": [
        "import torch \n",
        "import numpy as np\n",
        "import scipy.stats as si\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import f1_score\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "INPUT_DIM = 9 * 20\n",
        "OUTPUT_DIM = 2\n",
        "\n"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5V0o627FZyo"
      },
      "source": [
        "SEQ_LENGTH=20\n",
        "\n",
        "\n",
        "class DataProccesing(data.Dataset):\n",
        "\n",
        "\n",
        "    def data_to_input(self,sequence, pos_or_neg):\n",
        "        mapping = {'A': 0, 'R': 1, 'N': 2, 'D': 3, 'C': 4, 'E': 5, 'Q': 6, 'G': 7, 'H': 8, 'I': 9, 'L': 10, 'K': 11,\n",
        "                  'M': 12, 'F': 13, 'P': 14, 'S': 15, 'T': 16, 'W': 17, 'Y': 18, 'V': 19}\n",
        "        map=np.zeros((9, 20))\n",
        "        for i, seq in enumerate(sequence):\n",
        "            map[i,mapping[seq]]+=1\n",
        "        map = map.flatten()\n",
        "        return np.concatenate([map, np.array([pos_or_neg])])\n",
        "\n",
        "\n",
        "    def Read_Data(self,filename, pos_or_neg):\n",
        "        file = open(filename, 'r')\n",
        "        lines=file.readlines()\n",
        "        DATA = np.zeros((len(lines), 181))\n",
        "        for i, line in enumerate(lines):\n",
        "            input = self.data_to_input(line.replace('\\n', ''), pos_or_neg)\n",
        "            DATA[i] = input\n",
        "        return DATA\n",
        "\n",
        "\n",
        "    # def bootstrap(self,DATA,size,NUMBER_OF_BATCHS):\n",
        "\n",
        "    #     new_DATA=np.zeros((size,181))\n",
        "    #     N=DATA.shape[0]\n",
        "    #     batch_size=N//NUMBER_OF_BATCHS\n",
        "    #     for i in range(NUMBER_OF_BATCHS):\n",
        "\n",
        "    #         random = np.random.randint(batch_size*i,batch_size*(i+1), size=size//NUMBER_OF_BATCHS)\n",
        "    #         new_DATA[((size//NUMBER_OF_BATCHS)*i):(size//NUMBER_OF_BATCHS)*(i+1), :] = DATA[random, :]\n",
        "    #     return new_DATA\n",
        "\n",
        "    def bootstrap(self,DATA,size,NUMBER_OF_BATCHS):\n",
        "\n",
        "        N=DATA.shape[0]\n",
        "        batch_size=N//NUMBER_OF_BATCHS\n",
        "        for i in range(NUMBER_OF_BATCHS):\n",
        "\n",
        "            random = np.random.randint(batch_size*i,batch_size*(i+1), size=size//NUMBER_OF_BATCHS)\n",
        "            DATA =np.vstack([DATA,DATA[random, :]])\n",
        "        return DATA\n",
        "\n",
        "\n",
        "    def DATA_pre_pros(self,filename_pos,filename_neg):\n",
        "\n",
        "        neg_data=self.Read_Data(filename_neg, 0)\n",
        "        pos_data=self.Read_Data(filename_pos, 1)\n",
        "        \n",
        "        neg_data_train = neg_data[:int(len(neg_data)*0.9)]\n",
        "        neg_data_test = neg_data[int(len(neg_data)*0.9):]\n",
        "        pos_data_train = pos_data[:int(len(pos_data)*0.9)]\n",
        "        pos_data_test = pos_data[int(len(pos_data)*0.9):]\n",
        "        #pos_data_train = self.bootstrap(pos_data_train, int(BOOTSTRAP_SIZE*0.9), int(NUMBER_OF_BATCHS*0.9))\n",
        "        #pos_data_test = self.bootstrap(pos_data_test, int(BOOTSTRAP_SIZE*0.1), int(NUMBER_OF_BATCHS*0.1))\n",
        "\n",
        "        final_data_train = np.concatenate([neg_data_train, pos_data_train])\n",
        "        np.random.shuffle(final_data_train)\n",
        "\n",
        "        final_data_test = np.concatenate([neg_data_test, pos_data_test])\n",
        "        np.random.shuffle(final_data_test)\n",
        "\n",
        "        return final_data_train, final_data_test\n",
        "\n",
        "\n",
        "\n",
        "    def shuffle_data(self,data_Xy):\n",
        "        np.random.shuffle(data_Xy)\n",
        "        return data_Xy[:,:180],data_Xy[:,-1]\n",
        "\n",
        "\n",
        "    def spike_seq(self,filename):\n",
        "\n",
        "        mapping = {'A': 0, 'R': 1, 'N': 2, 'D': 3, 'C': 4, 'E': 5, 'Q': 6, 'G': 7, 'H': 8, 'I': 9, 'L': 10, 'K': 11,\n",
        "                  'M': 12, 'F': 13, 'P': 14, 'S': 15, 'T': 16, 'W': 17, 'Y': 18, 'V': 19}\n",
        "\n",
        "\n",
        "        with open(filename) as f:\n",
        "            lines = f.readlines()[0]\n",
        "            print(lines)\n",
        "            predeict=list()\n",
        "\n",
        "            if len(lines) == 9:\n",
        "                map = np.zeros((9, 20))\n",
        "                for i, seq in enumerate(lines):\n",
        "                    map[i, mapping[seq]] += 1\n",
        "                map = map.flatten()\n",
        "                predeict.append(map)\n",
        "            else:\n",
        "                for i in range(len(lines)-9):\n",
        "                    map = np.zeros((9, 20))\n",
        "                    for i, seq in enumerate(lines[i:i+9]):\n",
        "                        map[i, mapping[seq]] += 1\n",
        "                    map = map.flatten()\n",
        "                    predeict.append(map)\n",
        "            \n",
        "            return np.array(predeict)\n",
        "\n",
        "\n"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HutCXePsE9KO"
      },
      "source": [
        "INPUT_1=10\n",
        "INPUT_2=10\n",
        "INPUT_3=5\n",
        "P_DROPOUT=0.4\n",
        "P_DROPOUT2=0.15\n",
        "\n",
        "class NetWork(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_fc = nn.Linear(input_dim, INPUT_1)\n",
        "        \n",
        "        self.hidden_1_fc = nn.Linear(INPUT_1, INPUT_2)\n",
        "        \n",
        "        self.hidden_2_fc = nn.Linear(INPUT_2, INPUT_3)\n",
        "        \n",
        "        self.output_fc = nn.Linear(INPUT_3, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(P_DROPOUT)\n",
        "        self.dropout2 = nn.Dropout(P_DROPOUT2) \n",
        "        self.batch_norm_1 = nn.BatchNorm1d(INPUT_1)\n",
        "\n",
        "        self.batch_norm_2 = nn.BatchNorm1d(INPUT_2)\n",
        "\n",
        "        self.batch_norm_3 = nn.BatchNorm1d(INPUT_3)\n",
        "\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "   \n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        x = x.view(batch_size, -1)\n",
        "\n",
        "        h_1 = F.relu(self.input_fc(x))\n",
        "       \n",
        "        h_1=self.batch_norm_1(h_1)\n",
        "\n",
        "        h_1=self.dropout(h_1)\n",
        "\n",
        "        h_2 = F.relu(self.hidden_1_fc(h_1))\n",
        "\n",
        "        #h_2=self.batch_norm_2(h_2)\n",
        "\n",
        "        #h_2=self.dropout(h_2)\n",
        "\n",
        "        h_3 = F.relu(self.hidden_2_fc(h_2))\n",
        "\n",
        "        h_3=self.batch_norm_3(h_3)\n",
        "\n",
        "        #h_3=self.dropout2(h_3)\n",
        "\n",
        "        y_pred = self.output_fc(h_3)\n",
        "\n",
        "       \n",
        "        \n",
        "        return y_pred, h_2\n"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1jysnYBFurA"
      },
      "source": [
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "# def calculate_accuracy(y_pred, y):\n",
        "#     top_pred = y_pred.argmax(1, keepdim = True)\n",
        "#     correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
        "#     acc = correct.float() / y.shape[0]\n",
        "#     return acc\n",
        "\n",
        "\n",
        "# def calculate_accuracy(y_pred, y):\n",
        "#     top_pred = y_pred.argmax(1, keepdim = True)\n",
        "    \n",
        "#     f1 = f1_score(y_pred,y)\n",
        "\n",
        "#     return f1\n",
        "\n",
        "def f1_loss(y_true, y_pred, is_training=False):\n",
        "\n",
        "  \n",
        "    assert y_true.ndim == 1\n",
        "    assert y_pred.ndim == 1 or y_pred.ndim == 2\n",
        "   \n",
        "    if y_pred.ndim == 2:\n",
        "        y_pred = y_pred.argmax(dim=1)\n",
        "       \n",
        "   \n",
        "    tp = (y_true * y_pred).sum().to(torch.float32)\n",
        "    tn = ((1 - y_true) * (1 - y_pred)).sum().to(torch.float32)\n",
        "    fp = ((1 - y_true) * y_pred).sum().to(torch.float32)\n",
        "    fn = (y_true * (1 - y_pred)).sum().to(torch.float32)\n",
        "   \n",
        "    epsilon = 1e-7\n",
        "   \n",
        "    precision = tp / (tp + fp + epsilon)\n",
        "    recall = tp / (tp + fn + epsilon)\n",
        "   \n",
        "    f1 = 2* (precision*recall) / (precision + recall + epsilon)\n",
        "    f1.requires_grad = is_training\n",
        "    return f1\n",
        "\n",
        "\n",
        "\n",
        "def train(model, iterator_x, iterator_y, optimizer, criterion, device):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for i, x in enumerate(iterator_x):\n",
        "        x = x.to(device)\n",
        "        y = iterator_y[i].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        y_pred, _ = model(x)\n",
        "        \n",
        "        loss = criterion(y_pred, y)\n",
        "\n",
        "        #acc = calculate_accuracy(y_pred, y)\n",
        "        acc=f1_loss(y, y_pred, is_training=False)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator_x), epoch_acc / len(iterator_x)\n",
        "\n",
        "\n",
        "def evaluate(model, iterator_x, iterator_y, criterion, device):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for i, x in enumerate(iterator_x):\n",
        "        x = x.to(device)\n",
        "        y = iterator_y[i].to(device)\n",
        "\n",
        "        y_pred, _ = model(x)\n",
        "        \n",
        "        loss = criterion(y_pred, y)\n",
        "\n",
        "        #acc = calculate_accuracy(y_pred, y)\n",
        "        acc=f1_loss(y, y_pred, is_training=False)\n",
        " \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator_x), epoch_acc / len(iterator_x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNHufHg2Gyz5",
        "outputId": "8a58da4d-dd72-4716-8bd9-b1793b10b582",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "filename_pos='/content/ex1_DL/resorces/pos_A0201.txt'\n",
        "filename_neg='/content/ex1_DL/resorces/neg_A0201.txt'\n",
        "\n",
        "my_ds = DataProccesing()\n",
        "train_set, test_set=my_ds.DATA_pre_pros(filename_pos,filename_neg)\n",
        "train_dataloader=data.DataLoader(dataset=train_set, batch_size=64,shuffle=True)\n",
        "test_dataloader= data.DataLoader(dataset=test_set, batch_size=64,shuffle=True )\n",
        "print(np.sum(train_set[:, -1]))\n",
        "\n",
        "# LEARNNING_RATE=0.00015\n",
        "\n",
        "def main():\n",
        "    model = NetWork(INPUT_DIM, 2)\n",
        "    print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "    optimizer = optim.Adam(model.parameters(),lr=LEARNNING_RATE)\n",
        "\n",
        "    # weights = [np.sum(train_set[:, -1])/train_set.shape[0], 1-np.sum(train_set[:, -1])/train_set.shape[0]] #as class distribution\n",
        "    # class_weights = torch.FloatTensor(weights)\n",
        "    # criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    criterion = criterion.to(device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    EPOCHS = 50\n",
        "\n",
        "    best_valid_loss = float('inf')\n",
        "    train_per_ep=[]\n",
        "    test_per_ep=[]\n",
        "    test_acc = 0\n",
        "    for epoch in range(EPOCHS):\n",
        "\n",
        "\n",
        "        train_x,train_y=my_ds.shuffle_data(train_set)\n",
        "        train_iterator_x = torch.from_numpy(train_x.astype('float32')).split(64)\n",
        "        train_iterator_y = torch.from_numpy(train_y.astype('int64')).split(64)\n",
        "\n",
        "        test_x,test_y=my_ds.shuffle_data(test_set)\n",
        "        test_iterator_x = torch.from_numpy(test_x.astype('float32')).split(64)\n",
        "        test_iterator_y = torch.from_numpy(test_y.astype('int64')).split(64)\n",
        "\n",
        "        train_loss, train_acc = train(model, train_iterator_x, train_iterator_y, optimizer, criterion, device)\n",
        "\n",
        "        test_loss, test_acc = evaluate(model, test_iterator_x, test_iterator_y,criterion, device)\n",
        "        train_per_ep.append(train_loss)\n",
        "        test_per_ep.append(test_loss)\n",
        "        \n",
        "        print(f'Epoch: {epoch + 1:02}')\n",
        "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')\n",
        "        print(f'Epoch: {epoch + 1:02}')\n",
        "        print(f'\\tTest Loss: {test_loss:.3f} | Test Acc: {test_acc * 100:.2f}%')\n",
        "\n",
        "    train_per_ep=np.array(train_per_ep)\n",
        "    test_per_ep=np.array(test_per_ep)\n",
        "    epocs=np.arange(1,EPOCHS+1)\n",
        "    plt.figure()\n",
        "    plt.plot(epocs,train_per_ep)\n",
        "    plt.plot(epocs,test_per_ep)\n",
        "    plt.legend(['train loss','test loss'])\n",
        "    plt.show()\n",
        "    return test_acc, np.mean(np.abs(np.array(train_per_ep) - np.array(test_per_ep)))"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17397.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heAumw8VTJUC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkTUKuGfTuIE",
        "outputId": "49159bf3-36a6-43d6-af6a-88f25d6c88b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        }
      },
      "source": [
        "learning_rate = np.linspace(0.0005, 0.01, num=10)\n",
        "input_1 = [20, 30, 40]\n",
        "input_2 = [10, 20, 30]\n",
        "input_3 = [5, 10, 15]\n",
        "p = [0.2, 0.35, 0.5] \n",
        "acc = list()\n",
        "overfit=list()\n",
        "\n",
        "all=np.load('/content/drive/MyDrive/Colab Notebooks/opt.npy',allow_pickle=True)\n",
        "i=0\n",
        "for rate in learning_rate:\n",
        "  for input_1_ in input_1:\n",
        "    for input_2_ in input_2:\n",
        "      for input_3_ in input_3:\n",
        "        for p_ in p:\n",
        "            LEARNNING_RATE = rate\n",
        "            INPUT_1 = input_1_\n",
        "            INPUT_2 = input_2_\n",
        "            INPUT_3 = input_3_\n",
        "            P_DROPOUT = p_ \n",
        "\n",
        "            if any((np.array([INPUT_1,INPUT_2,INPUT_3,P_DROPOUT,LEARNNING_RATE])==all[:,0:-2]).all(1)):\n",
        "              print(i)\n",
        "              i+=1\n",
        "              continue\n",
        "            else:  \n",
        "              ac,of = main()\n",
        "              acc.append(ac)\n",
        "              overfit.append(of)\n",
        "              params=np.array([INPUT_1,INPUT_2,INPUT_3,P_DROPOUT,LEARNNING_RATE, ac, of])\n",
        "              all=np.vstack([all, params])\n",
        "              np.save('/content/drive/MyDrive/Colab Notebooks/opt.npy',all)\n",
        "          \n",
        "\n",
        "        \n",
        "\n"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-143-e184074c6fda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0moverfit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Colab Notebooks/opt.npy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/opt.npy'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZWo5iKwKtfD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "25065b7a-5835-4bb6-8ab2-1a1fc93acf73"
      },
      "source": [
        "filename_pos='/content/ex1_DL/resorces/pos_A0201.txt'\n",
        "filename_neg='/content/ex1_DL/resorces/neg_A0201.txt'\n",
        "\n",
        "my_ds = DataProccesing()\n",
        "train_set, test_set=my_ds.DATA_pre_pros(filename_pos,filename_neg)\n",
        "train_dataloader=data.DataLoader(dataset=train_set, batch_size=64,shuffle=True)\n",
        "test_dataloader= data.DataLoader(dataset=test_set, batch_size=64,shuffle=True )\n",
        "print(np.sum(train_set[:, -1]))\n",
        "\n",
        "LEARNNING_RATE=0.0004\n",
        "BOOTSTRAP_SIZE=15000\n",
        "NUMBER_OF_BATCHS=150\n",
        "\n",
        "model = NetWork(INPUT_DIM, 2)\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "optimizer = optim.Adam(model.parameters(),lr=LEARNNING_RATE)\n",
        "\n",
        "weights = [train_set.shape[0]/(2*np.sum(train_set[:, -1])), np.sum(train_set[:, -1])/(2*(train_set.shape[0]-np.sum(train_set[:, -1])))] #as class distribution\n",
        "\n",
        "weights.reverse()\n",
        "print(weights)\n",
        "class_weights = torch.FloatTensor(weights)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "#criterion = nn.CrossEntropyLoss()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "EPOCHS = 40\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "train_per_ep=[]\n",
        "test_per_ep=[]\n",
        "test_acc = 0\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "\n",
        "    train_x,train_y=my_ds.shuffle_data(train_set)\n",
        "    train_iterator_x = torch.from_numpy(train_x.astype('float32')).split(64)\n",
        "    train_iterator_y = torch.from_numpy(train_y.astype('int64')).split(64)\n",
        "\n",
        "    test_x,test_y=my_ds.shuffle_data(test_set)\n",
        "    test_iterator_x = torch.from_numpy(test_x.astype('float32')).split(64)\n",
        "    test_iterator_y = torch.from_numpy(test_y.astype('int64')).split(64)\n",
        "\n",
        "    train_loss, train_acc = train(model, train_iterator_x, train_iterator_y, optimizer, criterion, device)\n",
        "\n",
        "    test_loss, test_acc = evaluate(model, test_iterator_x, test_iterator_y,criterion, device)\n",
        "    train_per_ep.append(train_loss)\n",
        "    test_per_ep.append(test_loss)\n",
        "    \n",
        "    print(f'Epoch: {epoch + 1:02}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')\n",
        "    print(f'Epoch: {epoch + 1:02}')\n",
        "    print(f'\\tTest Loss: {test_loss:.3f} | Test Acc: {test_acc * 100:.2f}%')\n",
        "\n",
        "train_per_ep=np.array(train_per_ep)\n",
        "test_per_ep=np.array(test_per_ep)\n",
        "epocs=np.arange(1,EPOCHS+1)\n",
        "print(np.mean(np.abs(np.array(train_per_ep) - np.array(test_per_ep))))\n",
        "plt.figure()\n",
        "plt.plot(epocs,train_per_ep)\n",
        "plt.plot(epocs,test_per_ep)\n",
        "plt.legend(['train loss','test loss'])\n",
        "plt.show()"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2691.0\n",
            "The model has 2,037 trainable parameters\n",
            "[0.061042555122039745, 4.595503530286139]\n",
            "Epoch: 01\n",
            "\tTrain Loss: 0.370 | Train Acc: 22.54%\n",
            "Epoch: 01\n",
            "\tTest Loss: 0.243 | Test Acc: 24.00%\n",
            "Epoch: 02\n",
            "\tTrain Loss: 0.237 | Train Acc: 25.36%\n",
            "Epoch: 02\n",
            "\tTest Loss: 0.203 | Test Acc: 27.14%\n",
            "Epoch: 03\n",
            "\tTrain Loss: 0.200 | Train Acc: 28.69%\n",
            "Epoch: 03\n",
            "\tTest Loss: 0.195 | Test Acc: 32.43%\n",
            "Epoch: 04\n",
            "\tTrain Loss: 0.176 | Train Acc: 33.28%\n",
            "Epoch: 04\n",
            "\tTest Loss: 0.148 | Test Acc: 38.67%\n",
            "Epoch: 05\n",
            "\tTrain Loss: 0.160 | Train Acc: 38.13%\n",
            "Epoch: 05\n",
            "\tTest Loss: 0.135 | Test Acc: 43.01%\n",
            "Epoch: 06\n",
            "\tTrain Loss: 0.151 | Train Acc: 42.14%\n",
            "Epoch: 06\n",
            "\tTest Loss: 0.131 | Test Acc: 45.37%\n",
            "Epoch: 07\n",
            "\tTrain Loss: 0.146 | Train Acc: 44.08%\n",
            "Epoch: 07\n",
            "\tTest Loss: 0.125 | Test Acc: 48.47%\n",
            "Epoch: 08\n",
            "\tTrain Loss: 0.142 | Train Acc: 45.68%\n",
            "Epoch: 08\n",
            "\tTest Loss: 0.120 | Test Acc: 48.92%\n",
            "Epoch: 09\n",
            "\tTrain Loss: 0.135 | Train Acc: 46.04%\n",
            "Epoch: 09\n",
            "\tTest Loss: 0.124 | Test Acc: 49.97%\n",
            "Epoch: 10\n",
            "\tTrain Loss: 0.133 | Train Acc: 47.44%\n",
            "Epoch: 10\n",
            "\tTest Loss: 0.129 | Test Acc: 50.59%\n",
            "Epoch: 11\n",
            "\tTrain Loss: 0.137 | Train Acc: 47.19%\n",
            "Epoch: 11\n",
            "\tTest Loss: 0.125 | Test Acc: 49.01%\n",
            "Epoch: 12\n",
            "\tTrain Loss: 0.127 | Train Acc: 48.34%\n",
            "Epoch: 12\n",
            "\tTest Loss: 0.123 | Test Acc: 51.11%\n",
            "Epoch: 13\n",
            "\tTrain Loss: 0.130 | Train Acc: 48.47%\n",
            "Epoch: 13\n",
            "\tTest Loss: 0.125 | Test Acc: 51.66%\n",
            "Epoch: 14\n",
            "\tTrain Loss: 0.129 | Train Acc: 47.83%\n",
            "Epoch: 14\n",
            "\tTest Loss: 0.135 | Test Acc: 47.32%\n",
            "Epoch: 15\n",
            "\tTrain Loss: 0.126 | Train Acc: 48.24%\n",
            "Epoch: 15\n",
            "\tTest Loss: 0.141 | Test Acc: 49.06%\n",
            "Epoch: 16\n",
            "\tTrain Loss: 0.122 | Train Acc: 49.14%\n",
            "Epoch: 16\n",
            "\tTest Loss: 0.115 | Test Acc: 50.66%\n",
            "Epoch: 17\n",
            "\tTrain Loss: 0.127 | Train Acc: 48.67%\n",
            "Epoch: 17\n",
            "\tTest Loss: 0.121 | Test Acc: 50.53%\n",
            "Epoch: 18\n",
            "\tTrain Loss: 0.126 | Train Acc: 49.24%\n",
            "Epoch: 18\n",
            "\tTest Loss: 0.127 | Test Acc: 48.19%\n",
            "Epoch: 19\n",
            "\tTrain Loss: 0.121 | Train Acc: 48.85%\n",
            "Epoch: 19\n",
            "\tTest Loss: 0.125 | Test Acc: 49.87%\n",
            "Epoch: 20\n",
            "\tTrain Loss: 0.117 | Train Acc: 49.53%\n",
            "Epoch: 20\n",
            "\tTest Loss: 0.124 | Test Acc: 49.67%\n",
            "Epoch: 21\n",
            "\tTrain Loss: 0.118 | Train Acc: 50.03%\n",
            "Epoch: 21\n",
            "\tTest Loss: 0.125 | Test Acc: 51.98%\n",
            "Epoch: 22\n",
            "\tTrain Loss: 0.117 | Train Acc: 50.16%\n",
            "Epoch: 22\n",
            "\tTest Loss: 0.137 | Test Acc: 51.54%\n",
            "Epoch: 23\n",
            "\tTrain Loss: 0.118 | Train Acc: 49.72%\n",
            "Epoch: 23\n",
            "\tTest Loss: 0.135 | Test Acc: 50.44%\n",
            "Epoch: 24\n",
            "\tTrain Loss: 0.120 | Train Acc: 49.27%\n",
            "Epoch: 24\n",
            "\tTest Loss: 0.140 | Test Acc: 50.86%\n",
            "Epoch: 25\n",
            "\tTrain Loss: 0.116 | Train Acc: 50.10%\n",
            "Epoch: 25\n",
            "\tTest Loss: 0.133 | Test Acc: 50.92%\n",
            "Epoch: 26\n",
            "\tTrain Loss: 0.119 | Train Acc: 49.03%\n",
            "Epoch: 26\n",
            "\tTest Loss: 0.118 | Test Acc: 50.47%\n",
            "Epoch: 27\n",
            "\tTrain Loss: 0.118 | Train Acc: 48.87%\n",
            "Epoch: 27\n",
            "\tTest Loss: 0.149 | Test Acc: 50.84%\n",
            "Epoch: 28\n",
            "\tTrain Loss: 0.110 | Train Acc: 50.45%\n",
            "Epoch: 28\n",
            "\tTest Loss: 0.148 | Test Acc: 52.50%\n",
            "Epoch: 29\n",
            "\tTrain Loss: 0.118 | Train Acc: 50.19%\n",
            "Epoch: 29\n",
            "\tTest Loss: 0.133 | Test Acc: 52.33%\n",
            "Epoch: 30\n",
            "\tTrain Loss: 0.116 | Train Acc: 50.09%\n",
            "Epoch: 30\n",
            "\tTest Loss: 0.134 | Test Acc: 51.44%\n",
            "Epoch: 31\n",
            "\tTrain Loss: 0.117 | Train Acc: 50.74%\n",
            "Epoch: 31\n",
            "\tTest Loss: 0.136 | Test Acc: 53.85%\n",
            "Epoch: 32\n",
            "\tTrain Loss: 0.112 | Train Acc: 51.33%\n",
            "Epoch: 32\n",
            "\tTest Loss: 0.152 | Test Acc: 51.41%\n",
            "Epoch: 33\n",
            "\tTrain Loss: 0.113 | Train Acc: 50.66%\n",
            "Epoch: 33\n",
            "\tTest Loss: 0.143 | Test Acc: 51.44%\n",
            "Epoch: 34\n",
            "\tTrain Loss: 0.113 | Train Acc: 50.81%\n",
            "Epoch: 34\n",
            "\tTest Loss: 0.130 | Test Acc: 52.02%\n",
            "Epoch: 35\n",
            "\tTrain Loss: 0.113 | Train Acc: 51.27%\n",
            "Epoch: 35\n",
            "\tTest Loss: 0.140 | Test Acc: 52.00%\n",
            "Epoch: 36\n",
            "\tTrain Loss: 0.110 | Train Acc: 51.36%\n",
            "Epoch: 36\n",
            "\tTest Loss: 0.141 | Test Acc: 50.57%\n",
            "Epoch: 37\n",
            "\tTrain Loss: 0.108 | Train Acc: 52.10%\n",
            "Epoch: 37\n",
            "\tTest Loss: 0.169 | Test Acc: 52.06%\n",
            "Epoch: 38\n",
            "\tTrain Loss: 0.111 | Train Acc: 51.65%\n",
            "Epoch: 38\n",
            "\tTest Loss: 0.153 | Test Acc: 51.23%\n",
            "Epoch: 39\n",
            "\tTrain Loss: 0.111 | Train Acc: 50.84%\n",
            "Epoch: 39\n",
            "\tTest Loss: 0.152 | Test Acc: 50.83%\n",
            "Epoch: 40\n",
            "\tTrain Loss: 0.110 | Train Acc: 50.68%\n",
            "Epoch: 40\n",
            "\tTest Loss: 0.139 | Test Acc: 51.21%\n",
            "0.02215802659189151\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rym39Yotw8Ls"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfftOmlOTnCL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c267e37b-ac10-4596-9d3c-3153838ebe9c"
      },
      "source": [
        "\n",
        "seq=my_ds.spike_seq('/content/ex1_DL/resorces/spike_acid.txt')\n",
        "print(seq)\n",
        "\n",
        "seq = torch.from_numpy(seq.astype('float32'))\n",
        "seq = seq.to(device)\n",
        "y_pred, _ = model(seq)\n",
        "z = y_pred\n",
        "my_softmax = nn.Softmax(dim=1)\n",
        "z = my_softmax(z)\n",
        "print(len(torch.where(z[:,1]>0.991)[0]))\n"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSSVLHSTQDLFLPFFSNVTWFHAIHVSGTNGTKRFDNPVLPFNDGVYFASTEKSNIIRGWIFGTTLDSKTQSLLIVNNATNVVIKVCEFQFCNDPFLGVYYHKNNKSWMESEFRVYSSANNCTFEYVSQPFLMDLEGKQGNFKNLREFVFKNIDGYFKIYSKHTPINLVRDLPQGFSALEPLVDLPIGINITRFQTLLALHRSYLTPGDSSSGWTAGAAAYYVGYLQPRTFLLKYNENGTITDAVDCALDPLSETKCTLKSFTVEKGIYQTSNFRVQPTESIVRFPNITNLCPFGEVFNATRFASVYAWNRKRISNCVADYSVLYNSASFSTFKCYGVSPTKLNDLCFTNVYADSFVIRGDEVRQIAPGQTGKIADYNYKLPDDFTGCVIAWNSNNLDSKVGGNYNYLYRLFRKSNLKPFERDISTEIYQAGSTPCNGVEGFNCYFPLQSYGFQPTNGVGYQPYRVVVLSFELLHAPATVCGPKKSTNLVKNKCVNFNFNGLTGTGVLTESNKKFLPFQQFGRDIADTTDAVRDPQTLEILDITPCSFGGVSVITPGTNTSNQVAVLYQDVNCTEVPVAIHADQLTPTWRVYSTGSNVFQTRAGCLIGAEHVNNSYECDIPIGAGICASYQTQTNSPRRARSVASQSIIAYTMSLGAENSVAYSNNSIAIPTNFTISVTTEILPVSMTKTSVDCTMYICGDSTECSNLLLQYGSFCTQLNRALTGIAVEQDKNTQEVFAQVKQIYKTPPIKDFGGFNFSQILPDPSKPSKRSFIEDLLFNKVTLADAGFIKQYGDCLGDIAARDLICAQKFNGLTVLPPLLTDEMIAQYTSALLAGTITSGWTFGAGAALQIPFAMQMAYRFNGIGVTQNVLYENQKLIANQFNSAIGKIQDSLSSTASALGKLQDVVNQNAQALNTLVKQLSSNFGAISSVLNDILSRLDKVEAEVQIDRLITGRLQSLQTYVTQQLIRAAEIRASANLAATKMSECVLGQSKRVDFCGKGYHLMSFPQSAPHGVVFLHVTYVPAQEKNFTTAPAICHDGKAHFPREGVFVSNGTHWFVTQRNFYEPQIITTDNTFVSGNCDVVIGIVNNTVYDPLQPELDSFKEELDKYFKNHTSPDVDLGDISGINASVVNIQKEIDRLNEVAKNLNESLIDLQELGKYEQYIKWPALAKAPFAAALAKAPFAAWYIWLGFIAGLIAIVMVTIMLCCMTSCCSCLKGCCSCGSCCKFDEDDSEPVLKGVKLHYT\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 1. 0.]]\n",
            "226\n"
          ]
        }
      ]
    }
  ]
}